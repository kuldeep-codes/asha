<!doctype html>
<html>
    <head>
        <title>MB_500_CRS</title>
        <meta charset='utf-8'/>
        <style>
body {
  background-color: black;
  color: white;
  margin-left: 25%;
  margin-right: 25%;
  font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
}

div{
  color: white;
}

img{
  margin-top: auto;
  margin-bottom: auto;
  max-width: 80%;

}

p{
  font-size: large;
  /* display: inline; */
}

li{
font-size: large;
}

.scrollmenu {
  overflow: auto;
  max-height: 55vh;
  white-space: nowrap;
  text-align: left;
}

table {
  border-collapse: collapse;
  width: 100%;
}

/* Style for table header cells */
th {
  border: 1px solid #ccc;
  padding: 8px;
  text-align: center;
}


/* Style for table body cells */
td {
  border: 1px solid #ccc;
  padding: 8px;
  text-align: center;
}
</style>
    </head>
    <body>
<p>Okay, let's dive into this. It's less about step-by-step instructions and more about understanding the <em>why</em> behind using these development tools in Dynamics 365, especially when you're prepping for a certification exam.</p>
<p>So, the big picture here is that you, as a Dynamics 365 developer, need a space to play around, experiment, and build without messing up the live system. That's where the downloadable virtual hard disk, or VHD, comes in. Think of it like having your own personal sandbox – a complete Dynamics 365 environment that lives on your computer, or a server that you control. It allows you to develop or test without risk of damaging anything.</p>
<p>This is super important because, let's be real, you don't want to be writing code directly on your company's production system. That's like trying to juggle chainsaws while riding a unicycle – it's just a bad idea. </p>
<p>The document walks through how to get this VHD up and running, covering downloading the file from Lifecycle Services, a central place for managing your Dynamics 365 environments. The process to set this up initially might seem detailed, but it's all aimed at getting you to a point where you have your own environment that you can completely control.</p>
<p>What's key here for the exam is understanding the <em>concept</em> of a local development environment. The steps, while important to know, are less crucial than understanding why you would use a VHD in the first place. The VHD helps you isolate your development efforts and avoid any unintentional errors on the main Dynamics 365 implementation.</p>
<p>Now, let's talk about the tools inside this environment. Visual Studio is where you'll do all your code creation and customizations. Think of it like the craftsman’s workshop. You’ll also need SQL Server Management Studio to check on your data. The text mentions you also might want to add other tools, such as Power BI desktop or other integrations. It is about having a complete environment that’s tailored to your development needs.</p>
<p>The text then goes into an extremely important concept – <em>label files</em>. Imagine you’re creating an interface that people all over the world will use. You can’t just hard code text – you need to create labels. These labels allow you to localize the interface for other languages. This is about making the user interface internationalized. You create the labels using the label wizard in Visual Studio.</p>
<p>The document also touches on <em>project management</em> in Visual Studio – how to create and build projects to keep your development organized. It's about keeping things clean so you don't get lost in your own code. You have to make sure you have the right model selected when you create your project because that is hard to change later.</p>
<p>Then, there is the idea of updating your VHD. Think of it like updating your computer. You get new versions of the VHD that include fixes and new functionality, and you need to keep your local environment up to date. This part highlights a point that's often tested: the update process for a local VHD is <em>similar</em> to how you deploy code in a cloud-hosted dev environment. This is good to remember. When you update a VHD, it has to be compiled and synced.</p>
<p>Finally, when working on VHDs and developing code, it is important to keep in mind that changes to the database require a database sync. And if you are testing code that interacts with specific data, you need to restore a database that is appropriate for the scenario that you are trying to recreate. You can't just restore directly from production to a dev environment; you would need to first go through a Tier 2 or UAT environment, and then export it.</p>
<p>So, in summary, don't get bogged down in the nitty-gritty steps. Focus on understanding <em>why</em> a VHD is important for Dynamics 365 development, how it fits into the development lifecycle, and the key tools you'll use inside it. The conceptual understanding of why you use tools like Visual Studio, label files, and projects is key. It’s about building a good foundation, not memorizing all the details.</p>
<p>Alright, let's break down this section on metadata, the Application Object Tree (AOT), and X++ programming concepts in Dynamics 365. It's packed with information, but we can make it understandable and relatable for your exam prep.</p>
<p>First, the AOT, or Application Object Tree, is the heart of the development environment. Think of it as a giant catalog of <em>everything</em> that makes up your Dynamics 365 system – tables, forms, code, everything. It's how Dynamics 365 organizes all its internal structures. This section is teaching you about how the AOT works. It has a <em>model view</em> that groups by models and a <em>classic view</em> that groups by type.</p>
<p>The key takeaway is, that you need to understand how to navigate and find your elements inside of the AOT because it’s the basis of developing in Dynamics 365. You're going to find that you use these views a lot. Knowing how to switch between the model view (organized by the specific model it’s part of) and classic view (organized by the type of object, like tables, forms, etc.) will help you find elements quickly.</p>
<p>The document also lists the different <em>types of metadata</em>, like extended data types (EDTs), enums, tables, and forms. These are the building blocks of the system. EDTs and enums help you define what types of values your fields should have (like a string for a name, or a drop-down list for a status). These are very important in defining custom fields. Tables are where your data lives, and forms are how users interact with that data. Understanding these different data types is super important for development.</p>
<p>Also, you'll see mention of data entities, which are used to integrate with other systems. Think of these as a bridge between Dynamics 365 and the outside world. The AOT allows you to define analytical elements, label files, reports, resources, configuration, security, references, and services. This shows the breadth of what you can control and customize.</p>
<p>Let's move on to building <em>deployment packages</em>. This is essentially packaging up your code to deploy to other environments. The process is started through Azure DevOps. Understanding the process of building, and then uploading the SDP to the Lifecycle Services asset library, is critical. You can also track your builds from Team Explorer in Visual Studio.</p>
<p>The document emphasizes the importance of synchronizing your database after you make changes to metadata. This action makes sure that Dynamics 365 knows how to store your changes. If you don't do this after making structural changes to the database, you can run into SQL errors. This is a point you definitely want to remember. You can also set database sync to run automatically when you build.</p>
<p>The <em>Element Designer</em> is where you'll create new elements. It is not enough just to create the new elements though, you also have to extend existing elements to fit your specific development needs. You'll right-click the project to create a new item or right-click an element to extend it. Forms have their own specialized designer with the different layouts and patterns.</p>
<p>Now, let's discuss X++. It’s the programming language for Dynamics 365. X++ has a memory management model that is very similar to .NET. This document is not here to teach you everything about X++. However, the document is important to understand how X++ is different. It has case-insensitivity, strong data access statements, tight integration with system metadata, and a strong extensibility story.</p>
<p>The language also shares many keywords with SQL. This is something that sets X++ apart from other languages. There is a tight connection between the code you write and how the data is accessed. It also works with other .NET Framework languages.</p>
<p>The document goes into the different data types you can use in X++. It lists both primitive and composite data types. Primitive data types include basic types like numbers (int, real), text (str), dates, enums, and more. There are also composite data types, such as arrays, containers, and tables. You can also define your own data types (EDTs), which can help you make more readable code.</p>
<p>Finally, the document touches upon variables. Variables are used to store the data you will work with. They can be declared anywhere in the method, and you do not have to declare them at the beginning.</p>
<p>In short, focus on understanding the overall structure of the AOT, the different types of metadata you'll be working with, how to deploy changes, the concepts behind X++, and the basic data types of that language. For example, when the text mentions "synchronize database," understand <em>why</em> that's important. When it talks about data types, understand the difference between an EDT and an enum. This conceptual understanding is critical for the exam. Don't sweat memorizing every specific step, focus on understanding the concepts at play.</p>
<p>Okay, let's unpack this section on X++ variables, operators, classes, collections, and SQL statements. It's a lot to take in, but we'll break it down into manageable parts, focusing on what's key for your exam.</p>
<p>First, let's talk about <em>variables</em>. Remember how we discussed data types? Well, variables are like containers that hold specific types of data. This section is about how you work with these containers in X++. Global variables (or member variables) are available across the whole class instance (except static methods), while local variables only live within the method where they're defined.</p>
<p>When you declare a variable, you need to specify its data type (like "str" for string) and give it a meaningful name (following camel case – like "customerName"). You can assign a value right away, or do it later. The point here is that variables are not just boxes but have specific characteristics.</p>
<p>The document also covers <em>operators</em>, which are symbols that perform actions on variables. You'll see assignment operators (like "=", "+=", "++"), which change a variable’s value. Arithmetic operators (like "+", "-", "*", "/") do math calculations. Relational operators (like "==", "!=", "&gt;", "&lt;") compare values. A critical operator for exam purposes is the ternary operator ("? :"). Remember this because it can often be in code snippets.</p>
<p>Next up, <em>classes</em>. These are blueprints for creating objects. A class has variables (the state of the object) and methods (what the object does). Classes have public, protected, and private modifiers that affect the accessibility. It's good to know that all classes are treated as public in X++, even if they are not explicitly declared as public. You need to instantiate the class to actually use it, and you can use both instance and static methods in classes. Instance methods are bound to a class object that you create, and you do not need to create an object to use static methods. Methods have a signature and a body.</p>
<p>Now, let's move on to <em>collection classes</em>. These are used to store and manipulate multiple pieces of data. They provide different ways to organize information. These collections provide reliable performance, and they can include primitive and object types.</p>
<p>Here is a breakdown:</p>
<ul>
<li><strong>List:</strong> A sequential collection of a single data type (you can add to the beginning or end of a list).</li>
<li><strong>Map:</strong> A collection of key-value pairs where each key maps to a value of a specified type.</li>
<li><strong>Set:</strong> A unique collection of a single data type (no duplicates allowed).</li>
</ul>
<p>Knowing the use cases for these is important for the exam. Lists are great for ordered items, maps for associating data, and sets for ensuring uniqueness. The document also gives examples of how to traverse each of these collections.</p>
<p>Lastly, <em>SQL statements in X++</em>. The document describes how you can retrieve data using select statements, filter the data with where clauses, and combine tables using joins. This is a critical section because you use these methods to interact with the database. A basic select statement can return an entire table, or you can add parameters to only return specific fields. You also can use aggregate functions like sum, avg, minof, maxof, or count to return aggregated values. When selecting data for update, it is important to add the "forUpdate" parameter so that a lock is placed on the record.</p>
<p>There are many methods of joining tables. The join keyword connects two tables and returns records that match the criteria. A while select statement will loop through every record that meets the select criteria.</p>
<p>The document then touches on <em>insert and update statements</em>. These methods are how you create and modify data in the database. A very important point here is that these methods use "ttsbegin" and "ttscommit" to ensure that data operations are treated as a single transaction. You can also use "insert_recordset" and "update_recordset" to do multiple operations at the same time.</p>
<p>Here is a summary of the main points:</p>
<ul>
<li><strong>Variables</strong>: Remember scope (global vs. local), data types, and naming conventions.</li>
<li><strong>Operators</strong>: Be familiar with assignment, arithmetic, relational, and the ternary operator.</li>
<li><strong>Classes</strong>: Understand modifiers (public, private, protected), instantiation, and instance vs. static methods.</li>
<li><strong>Collection Classes</strong>: Know the differences between List, Map, and Set.</li>
<li><strong>SQL in X++</strong>: Understand select, where, join, insert, and update statements and how to use them.</li>
</ul>
<p>Don't get bogged down in every detail. Focus on understanding the <em>concepts</em> behind each element, how they are used, and what role they play in Dynamics 365.</p>
<p>Okay, let's tackle the final part of this module – focusing on the X++ concepts of conditional and iterative statements, exception handling, global functions, and a few additional details to make sure you’re well-prepared.</p>
<p>First, let's talk about <em>delete methods</em> in X++. They're used to remove records from a table. Just like insert and update methods, delete methods need to be wrapped in "ttsbegin" and "ttscommit" blocks to make sure the delete operation is a single transaction. Always remember that. You can use "doDelete()" to override the standard delete method behavior.</p>
<p>Next, we have <em>conditional statements</em>, which are used to control the flow of your code. Think of them as "decision makers."</p>
<p>Here are the key conditional statements:</p>
<ul>
<li><strong>"if" statement</strong>: Runs code only if a condition is true.</li>
<li><strong>"if...else" statement</strong>: Runs one block of code if the condition is true and a different block if it's false.</li>
<li><strong>"switch" statement</strong>: A multi-branch conditional statement that checks a single value against multiple cases. It uses "break" statements to prevent fall through to the next case.</li>
<li><strong>Ternary operator</strong>: A way to concisely check a single condition that is like a short if statement.</li>
</ul>
<p>The document also goes into <em>iterative statements</em> (loops) that repeat a block of code.</p>
<p>Here are the key loops:</p>
<ul>
<li><strong>"while" loop</strong>: Runs a block of code as long as a condition is true.</li>
<li><strong>"do...while" loop</strong>: Like a while loop, but the code runs at least once before the condition is checked.</li>
<li><strong>"for" loop</strong>: Useful for iterating a specific number of times; often used with collections or arrays.</li>
</ul>
<p>Remember these terms and their appropriate use cases for the exam. Also, know when and how to use a "break" (to exit a loop) or "continue" (to skip to the next iteration) statement.</p>
<p>Now, let's dive into <em>exception handling</em>. This is how you deal with errors in your code. Exception handling includes the following:</p>
<ul>
<li><strong>"throw" statement</strong>: Used to signal an error.</li>
<li><strong>"try...catch" statement</strong>: Used to run code that might throw an exception (try block) and handle the error (catch block). You can catch a numeric exception by including "Exception::Numeric" inside of the catch parenthesis.</li>
<li><strong>"finally" block</strong>: Runs code that needs to be executed regardless of whether an error occurred or not, such as closing a file.</li>
<li><strong>"retry" statement</strong>: You can use the retry statement in the catch statement to rerun the code inside the try statement.</li>
</ul>
<p>It’s crucial to understand how these work together. When an error occurs, the program execution jumps to the catch block. If the code can correct the error, then a retry statement can be useful. Make sure that the retry does not create an infinite loop, and all finally code will be run. You also need to know that exceptions inside transactions will cause the transaction to abort automatically.</p>
<p>The document also mentions using "Message()" API for more control over messages, and the "Message::AddAction()" method to add actions directly within the message bar. You should also know that if an exception is not handled, then the call to the current method is unraveled and the exception will be handled, or not handled, in the caller's scope.</p>
<p>Next, there is the idea of <em>constant values</em>. You can declare a constant variable with the "const" keyword. Once declared, the value cannot be changed.</p>
<p>Let's talk about <em>global functions</em>. You can create them as static methods in the "Global" class. Global functions can be used from anywhere in your code. Remember, you call global functions by only using their name, not the class name. It's good practice to extend existing global functions by using the Chain of Command pattern, which is shown in the example.</p>
<p>The text also briefly mentions using X++ runtime functions for common tasks, such as financial, string, and date operations.</p>
<p>Finally, it discusses calling <em>.NET libraries</em> from X++. You can add a reference to a C# class library project in your Dynamics 365 project.</p>
<p>Here's a quick recap of the key concepts:</p>
<ul>
<li><strong>Delete Methods</strong>: Use ttsbegin and ttscommit.</li>
<li><strong>Conditional Statements</strong>: Know "if", "if...else", "switch", and ternary operators.</li>
<li><strong>Iterative Statements</strong>: Know "while", "do...while", and "for" loops.</li>
<li><strong>Exception Handling</strong>: Understand "throw", "try...catch", "finally", and "retry".</li>
<li><strong>User Messages</strong>: Know the purpose of "Message()" and "Message::AddAction()".</li>
<li><strong>Constant Values</strong>: Declare a const variable.</li>
<li><strong>Global Functions</strong>: Create them in the Global class as static methods.</li>
<li><strong>.NET Libraries</strong>: You can call these from within your X++ code.</li>
</ul>
<p>For your exam, focus on the practical application of these concepts. How would you use a "while" loop versus a "for" loop? When would you use a "switch" statement instead of nested "if" statements? How do you handle exceptions to prevent your code from crashing? How do you declare global methods and use them in your code?</p>
<p>Okay, let's wrap up this module on object-oriented programming (OOP) concepts in X++, focusing on inheritance, abstract classes, attributes, Chain of Command (CoC), scoping, and interfaces. This is a dense section, so we'll go through it systematically.</p>
<p>First, let's tackle <em>inheritance</em>. Think of it as building upon existing work. You create a base class (the parent), and then you create derived classes (the children) that inherit the parent's properties and methods.</p>
<p>Here's the key thing:</p>
<ul>
<li>The derived class (child) gets all the stuff from the parent class, and it can also override or add to that functionality.</li>
<li>Inheritance allows you to avoid writing duplicate code, promoting reuse.</li>
</ul>
<p>The example of vehicles illustrates this concept perfectly. A car, bus, and truck are all types of vehicles, so they inherit common characteristics from the base "Vehicle" class (like height and width), but each has its own unique features.</p>
<p>Next up is <em>abstract classes</em>. These are classes that can't be directly created (instantiated). They're used to create a template for other classes. Abstract methods are also defined within abstract classes, and those methods do not have a body, or a code implementation. These methods <em>must</em> be implemented by the classes that inherit the abstract class. It’s like saying, "Every class that extends this <em>must</em> have this method." In other words, it is like a contract.</p>
<p>Next, there are <em>attributes</em>. Attributes are used to provide metadata, or extra information, about code. You can add attributes in square brackets before classes or methods. Attributes can be used in many ways, for example, to mark something as obsolete or to control a method’s behavior. You can also create your own attributes by extending "SysAttribute". It’s about adding extra context or rules for the compiler.</p>
<p>One important example of attributes is using the "SysObsoleteAttribute" on methods or classes that should no longer be used. During the build process, you will be notified in the output window. The first parameter is the message, and the second parameter indicates whether the obsolete method should be a warning or an error.</p>
<p>Now, let's discuss one of the most important concepts in Dynamics 365 development: <em>Chain of Command (CoC)</em>. This is a specific way to extend existing code without directly modifying the original code. CoC uses class extensions and is not used with event handlers. You create an extension class, and the name must be the same as the original class but with "_Extension" as a suffix. You can add code that runs <em>before</em> (by adding code before the call to "next") or <em>after</em> (by adding code after the call to "next") the original code. This allows you to add custom logic on top of existing methods. You also use the "ExtensionOf" attribute when declaring the class.</p>
<p>Keep in mind that certain methods cannot be wrapped, such as methods tagged with "[Hookable(false)]" or "[Wrappable(false)]", methods that use the final keyword, and private methods. Only public and protected methods are available for extensions. The "next" keyword passes execution to the original method, and when no more methods exist in the chain, the original method is called.</p>
<p>Let's move on to <em>scoping</em> and <em>access identifiers</em>. Scoping determines where you can access a variable or method in your code.</p>
<p>Access identifiers control the visibility of your code:</p>
<ul>
<li><strong>"public"</strong>: Accessible from anywhere. You can override it from a subclass.</li>
<li><strong>"protected"</strong>: Accessible within the class and subclasses. You can override it from a subclass.</li>
<li><strong>"private"</strong>: Accessible only within the same class. You cannot override it.</li>
<li><strong>"internal"</strong>: Accessible only from within the same model.</li>
</ul>
<p>Instance variables (fields) are declared within the class and can be accessed from the class itself. Local variables are declared inside a method and are only available in that method.</p>
<p>Parameters are a specific type of local variable that are passed to a method, and they are used to pass values between scopes. Parameters use the term "call by value", meaning that you do not change the original value of the variable, and you only change the local variable. It is important to remember this for the exam because this can be a common error in code.</p>
<p>Finally, let's explore <em>interfaces</em>. Interfaces define a "contract" that classes can follow. An interface defines a set of public methods that any class that implements it must contain. They cannot include code implementations of methods. A class can implement multiple interfaces, and an interface can extend another interface, but not more than one.</p>
<p>Here's a quick summary of the key takeaways:</p>
<ul>
<li><strong>Inheritance</strong>: Understand parent and child classes, and how child classes can extend or override functionality.</li>
<li><strong>Abstract Classes</strong>: Know that they can’t be instantiated and need subclasses.</li>
<li><strong>Attributes</strong>: Know how to mark code as obsolete and define other metadata.</li>
<li><strong>Chain of Command (CoC)</strong>: Know how to extend existing code.</li>
<li><strong>Scoping</strong>: Understand the access identifiers and the difference between local and instance variables.</li>
<li><strong>Interfaces</strong>: Understand how interfaces enforce similarities among classes and how classes can implement multiple interfaces.</li>
</ul>
<p>For the exam, focus on understanding <em>why</em> these OOP concepts are used and how they contribute to building flexible, maintainable, and extensible Dynamics 365 solutions.</p>
<p>Okay, let's break down this section on Application Lifecycle Management (ALM) methodologies, models, build processes, testing, version control, upgrades, and risk management. It's a broad topic, but we can simplify it and focus on the key concepts for your exam.</p>
<p>First, let's talk about ALM methodologies. ALM is the process of managing an application from its inception to retirement. The document goes over three methodologies: Waterfall, Agile, and Spiral.</p>
<ul>
<li><strong>Waterfall:</strong> This is a sequential approach. The project is divided into distinct phases that flow from one to the next. Each phase is documented and needs to be completed before the next one begins. Use the waterfall methodology when the requirements are simple, defined, and unlikely to change, and the project is released all at once.</li>
<li><strong>Agile:</strong> This is an iterative approach that focuses on continuous feedback and change. Agile is typically organized into sprints and is a good choice when requirements are unclear, changes are expected during the lifecycle of the project, and when the project does not have to be released all at once. It can be difficult to track and requires good communication and coordination.</li>
<li><strong>Spiral:</strong> Combines elements of Waterfall and Agile. This approach is focused on risk assessment. It breaks the project into smaller parts, and each part follows a sequence of steps similar to the waterfall approach, but then it plans the next iteration. Use the spiral methodology for large complex systems.</li>
</ul>
<p>The key takeaway is understanding when to use each methodology. Waterfall is good for well-defined, unchanging projects; Agile is better for projects with uncertainty and changing requirements, and Spiral for large complex systems with risk assessments.</p>
<p>Next, we need to understand the importance of <em>models</em>. In Dynamics 365, a model is a grouping of elements (metadata and source files) that define your solution. Think of it like a container for your customizations. Models are essential because customizations are created inside models. Each project can only be assigned to one model. Models can be grouped into deployable packages that are used to promote code from one environment to another. You can create a model that is in its own package or part of an existing package. It is recommended to not overlay any code in an existing package. The document also mentions that you can change the model parameters from the Dynamics 365 menu.</p>
<p>The document then goes into how to plan the <em>build, test, and quality control processes</em>. It mentions that the build process varies based on the chosen methodology. It also emphasizes the need to plan for when things don't pass the test, and you might need to roll back certain changes. It is very important to test all code before moving it to production.</p>
<p>It also explains how to plan which environments are needed. It explains the differences between Tier-1 (single-box, for development) and Tier-2 (multi-box, like production) environments. The standard cloud offer includes a Tier-1 for development, a Tier-2 for UAT, and a production environment. You might also use other add-on or cloud-hosted environments. Always, remember that you should choose your environment based on its purpose.</p>
<p>The document also explains the need for <em>testing</em>. There are three primary types of testing:</p>
<ul>
<li><strong>Unit Testing:</strong> Checks specific functionality.</li>
<li><strong>Regression Testing:</strong> Checks if new code interferes with existing features.</li>
<li><strong>Performance Testing:</strong> Tests system stability under heavy use.</li>
</ul>
<p>It is also important to know how to use the Task recorder tool to document user processes.</p>
<p>Next, the document discusses <em>source control</em>. A source control system is required to prevent conflicts among multiple developers who are working simultaneously in finance and operations apps.</p>
<p>Source control is used for version control. It allows developers to check in and out objects. When an object is checked out, it is noted that the developer is making changes to that object. Then, when the changes are checked in, the system stores a new version of that object. Source control systems allow you to review, revert to, and merge changes that multiple developers might be working on at the same time.</p>
<p>There are two primary version control systems that can be used with finance and operations apps:</p>
<ul>
<li><strong>Team Foundation Version Control (TFVC):</strong> Is a centralized version control system. Each developer works in one branch and has one version of each file. You can activate or deactivate TFVC repositories in the organization settings.</li>
<li><strong>Git:</strong> Is the recommended default version control system by Microsoft and is a distributed system where developers can have their own copy of the source repository (or a lightweight branch). Git makes it much easier to switch between different versions of the code.</li>
</ul>
<p>The key takeaway is that <em>you must use</em> source control, and Git is the recommended version control system by Microsoft.</p>
<p>The document also discusses how to identify <em>upgrade scenarios</em>. The document talks about the different processes to upgrade from AX 2012, apply updates to an on-premises version, and apply updates to a cloud version of finance and operations apps. It breaks down an upgrade from AX 2012 into the phases: Analyze, Execute, and Validate.</p>
<p>Updates are applied to on-premises environments by using deployable packages from Lifecycle Services. Upgrades are also started manually in Lifecycle Services.</p>
<p>The document also explains the need for <em>release, change, and risk management</em>. Remember that Microsoft releases new updates regularly, and it is recommended that you always move to the latest version to avoid compatibility issues. This can include new features or bug fixes. The document also explains that you should read the release plans to determine the areas that might be affected. It also emphasizes performing a code review to identify issues early on. You need to identify risk and assign a business process owner to be responsible for specific risk categories. Some risks that might need to be considered include system downtime, resource availability, and the need for additional development. You might also consider using the Regression Suite Automation Tool (RSAT) to automate testing based on task recordings.</p>
<p>In summary, understand:</p>
<ul>
<li><strong>ALM Methodologies</strong>: Waterfall, Agile, and Spiral and their use cases.</li>
<li><strong>Models</strong>: How they organize customizations, and the differences between models that are in their own package, and part of an existing package.</li>
<li><strong>Builds</strong>: How to plan the build processes and roll back errors.</li>
<li><strong>Environments</strong>: The purpose of Tier-1 and Tier-2 environments.</li>
<li><strong>Testing</strong>: Unit, regression, and performance testing.</li>
<li><strong>Source Control</strong>: The differences between Git and TFVC.</li>
<li><strong>Upgrades</strong>: Know how to analyze the upgrade path.</li>
<li><strong>Risk Management</strong>: Identify potential risks and plan to mitigate them.</li>
</ul>
<p>Focus on understanding the <em>why</em> behind these processes and how they ensure a smooth development lifecycle.</p>
<p>Alright, let's wrap up this final section on Extended Data Types (EDTs), base enums, tables, and data models. It's a critical area for Dynamics 365 development, so let's make sure we've got a solid grasp of the core concepts.</p>
<p>First, let's start with a question that you might see in the exam: "A new developer is joining the project team. You, as the project manager, must request a new environment for the developer to use. What kind of environment should you request?" The answer is a <strong>Tier 1 environment.</strong> Tier 1 is a single box environment used for development and testing. Tier-2 and production environments should be used for UAT and production.</p>
<p>Now, let's talk about EDTs and base enums. These are all about making your data consistent, reusable, and easier to manage. Think of them as the building blocks for all the fields that users interact with.</p>
<ul>
<li><strong>EDTs (Extended Data Types):</strong> These are custom data types based on the primitive data types like strings, integers, or dates. They allow you to add more specific properties to those types, which are reusable across the system. For instance, instead of using the primitive data type "string," you might create an EDT named "CustomerName" and set certain length or format properties for that EDT. If you declare a variable that is based on an EDT, the variable will use the properties that are specified for that EDT. This helps to reduce the number of data type conversions, improves data integrity, and creates a more consistent experience across the application. The text highlights this point about using EDTs to reuse functionality, improve code readability, and create hierarchies that inherit properties from a parent EDT.</li>
<li><strong>Base Enums:</strong> These are a predefined set of values (literals). Each value is represented internally as an integer, which saves space in the database, and has a corresponding display name that is used in the UI. When creating a base enum, it is best practice to make the first literal the value of 0, and so on. These literals can be used as integers in X++ code. They are used in the user interface as options, such as a drop-down menu or option buttons. The document also states how to add base enums to a project and how they are added to a form.</li>
</ul>
<p>The main point is that EDTs are good for enforcing data rules and base enums are good for providing lists of options to the user. Also, base enums are stored as numeric values in the database, not as string values.</p>
<p>Now, let's dive into <em>tables</em>. Tables are how Dynamics 365 organizes data. They are composed of rows and columns and contain fields, field groups, indexes, relations, delete actions, state machines, mappings, methods, and events.</p>
<p>Key concepts to remember about tables:</p>
<ul>
<li><strong>Fields</strong>: These are the columns in the table and are usually based on EDTs or base enums.</li>
<li><strong>Field groups:</strong> Logical groupings of fields, so related objects are automatically updated when the field group changes.</li>
<li><strong>Indexes:</strong> Help to make data retrieval faster.</li>
<li><strong>Relations:</strong> How data in one table is connected to data in another.</li>
<li><strong>Delete Actions:</strong> What should happen when data in a related table is deleted.</li>
<li><strong>State Machines:</strong> Used with workflows.</li>
<li><strong>Mappings:</strong> Used to connect map fields to table fields.</li>
<li><strong>Methods</strong>: Pieces of code that are run on certain events.</li>
<li><strong>Events</strong>: Pieces of code that are run before or after a base method.</li>
</ul>
<p>You should know how to use the table designer and add new fields, field groups, indexes, and relations. The text emphasizes how to check if an EDT already exists before creating a new EDT and adding it to your project, which promotes reusability.</p>
<p>The document also talks about how to populate table and field <em>properties</em>. You'll need to understand the purpose of properties like table type (regular or temporary), name, label, primary index, cluster index, configuration key, support inheritance, and extends. Table fields do not have as many adjustable properties, and most properties are modified on the base enums or EDTs themselves. For example, if you add an EDT as a field to a table, you would open the EDT in the element designer to adjust any properties before adding it to the table.</p>
<p>Here's a quick recap of what you should focus on:</p>
<ul>
<li><strong>EDTs</strong>: Know their purpose, their benefits, and how to use them.</li>
<li><strong>Base Enums</strong>: Know how to use them, what they are, how they are stored, and the different ways that you might see them in the user interface.</li>
<li><strong>Tables</strong>: Understand the core elements (fields, indexes, relations, etc.), how they are used, and how you can create and modify them.</li>
<li><strong>Properties</strong>: Understand table properties like table type, name, label, primary index, and cluster index.</li>
</ul>
<p>For your exam, focus on the practical applications of EDTs, base enums, and tables. When would you create a new EDT versus using an existing one? Why are relations between tables important? When would you use an index on a table, and what is the difference between primary and cluster indexes? How would you ensure that your EDTs and Base Enums follow best practices?</p>
<p>Okay, let's break down this section on properties, adding fields, field groups, indexes, relations, table methods, views, and computed columns. It's a lot, but we can simplify it by focusing on the core concepts.</p>
<p>First, let's talk about <em>base enum and EDT properties</em>. The text explains that the properties for base enums determine if they display as a drop-down menu, option buttons, or a slider bar. For EDTs, the properties vary based on the type of EDT. For example, a string EDT can control how many characters a user can input. And a date EDT can control how the date is displayed. The text recommends using the "Auto" setting for properties so that the system can adjust the field as necessary in the user interface. This will enable the UI to adapt to the specific form pattern.</p>
<p>Now, let's discuss how to <em>add fields, field groups, indexes, and relations to tables</em>. When you are adding fields to the table, you drag and drop them from the AOT or your project. If you do not add fields to a field group, then only two fields are shown in the primary table (the primary key and the description field) because those are automatically indexed.</p>
<ul>
<li><strong>Field groups</strong> are a way to pool associated fields. They increase consistency in the form design, reduce the number of clicks a user needs to perform, and can automatically update forms when the group is modified. Field groups are added to a table through the Table designer window. When a change is made to a field group, any form that uses that field group will automatically be updated.</li>
<li><strong>Indexes</strong> help to speed up data retrieval. The text emphasizes considering the fields that are often searched, used in joins, or used to order results when choosing what fields to use for the index. The order of the columns is also important, from the most unique to the least unique. Also, remember there are different types of indexes: Primary, clustered, and non-clustered.
<ul>
<li><strong>Primary</strong>: Provides a unique identifier for each row and cannot have duplicate values.</li>
<li><strong>Clustered</strong>: Organizes the physical storage of the data based on the index values.</li>
<li><strong>Non-clustered</strong>: Used to find data based on other specified columns.</li>
</ul>
</li>
<li><strong>Table relations</strong> are used to define how tables are related to each other. They can be used to create auto joins in forms, look up values, and validate data. Table relations have a few key properties to understand:
<ul>
<li><strong>Cardinality</strong>: Defines the relationship between the tables (e.g., one-to-many, one-to-one). The relation between a header table and lines could be ZeroMore, meaning one header exists and has zero-to-many lines. The relation between lines and a header is ExactlyOne, and a header always exists.</li>
<li><strong>On Delete</strong>: Defines how deletions in the main table are handled in related tables. You can choose if the system should restrict the delete (when data exists in the related table), cascade the delete (so the related records are also deleted), or make it a CascadeRestricted delete (so that it selects which action is appropriate, depending on the table).</li>
<li><strong>Relationship Type</strong>: Specifies the type of relationship (e.g., association, composition, link). The relationship types are listed in the text, and you can reference it to learn when it is appropriate to use them. For example, a link is used in migration tools from older versions, and you should not use it in new code. Composition is used if the current relation only exists once. Aggregation is used if the parent table has a delete action that should be used in the current relation, and association is used for a standard foreign key relation.</li>
</ul>
</li>
</ul>
<p>Let's move on to <em>table methods</em>. Tables have standard methods that are called during certain events, and you can customize or extend those methods. The document mentions the following methods:</p>
<ul>
<li>"initValue()": Called when a new record is created.</li>
<li>"find()": Usually a custom method that is created to return a specific record.</li>
<li>"insert()": Called when a new record is inserted.</li>
<li>"modifiedField()": Called when a field is modified.</li>
<li>"validateField()": Called to validate data entered into a field.</li>
<li>"validateWrite()": Called when the user saves the record or leaves the form.<br>
It is important to remember that you need a "super()" call in the standard methods to call the framework logic, and you can extend these methods by using the chain of command.</li>
</ul>
<p>The text also provides an example of how to extend the initValue method with a chain of command.</p>
<p>Finally, the text discusses <em>views</em>. Views have rows and columns, like tables, but views are based on a query and do not hold the data themselves. Instead, they display data from joined tables. Like tables, views have many of the same components: fields, field groups, indexes, relations, mappings, state machines, methods, and events. The most important property that is specific to views is the "Query" property. A view can be based on an existing query, or you can manually create the view metadata. Views are created through the Solution Explorer window. You can create view extensions, but you can not create code extensions. Use event handlers for pre and post events for methods.</p>
<p>The text also discusses the use of a computed column (or virtual field) in a view. These are calculated by using a method that returns T-SQL. The document also provides an example of how to use "DictView" to return the fields required to build that T-SQL statement. Computed columns are a way to display calculations that are performed directly on the database instead of within the application. You have to sync views to the database to create the view in the underlying database.</p>
<p>Here's a summary of the key points to remember:</p>
<ul>
<li><strong>Base Enums and EDT Properties</strong>: Understand how they control the user interface.</li>
<li><strong>Adding Fields and Field Groups</strong>: Know how to organize fields and maintain consistency.</li>
<li><strong>Indexes</strong>: How to speed up data retrieval using indexes.</li>
<li><strong>Table Relations</strong>: How to define relationships between tables.</li>
<li><strong>Table Methods</strong>: Understand the different types of table methods and how to extend them using chain of command.</li>
<li><strong>Views</strong>: How they organize data and the difference between tables and views.</li>
<li><strong>Computed Columns</strong>: How to implement computed fields using T-SQL.</li>
</ul>
<p>For your exam, focus on the practical aspects of these concepts. You should understand the purpose of each element and how they can be used for a specific task. For example, when should you use field groups, what should be added to an index, and what types of relations can be used? You should know how to create methods and extend them by using chain of command. Finally, you should understand the difference between tables and views, and when you would want to use each.</p>
<p>Okay, let's break down this final section on extending views, creating/managing/extending queries, and creating/managing/extending table maps. This is all about working with different ways to display and organize data in Dynamics 365, so let's get it clear.</p>
<p>First, the document talks about <em>extending views</em>. The text emphasizes that, when you are extending views, you cannot add computed columns because you cannot add code. You can, however, change the metadata of a view by adding a range or new data source. You can also add fields, change field groups, or add new mappings. If you need custom code, you need to create an event handler class for methods. You extend views by right-clicking the view in the application explorer and selecting "Create Extension."</p>
<p>Next, the text moves on to <em>queries</em>. The text states that queries are used to build SQL statements and that it is best practice to use them in forms, views, and reports to reduce the size of the SQL statement that’s run and increase performance. Queries are used to predefine how the data is displayed and organized in finance and operations apps.</p>
<p>To create a query, go to Solution Explorer, right-click the project, select Add &gt; New Item and Data Model &gt; Query. Queries have some top-level properties to be aware of, including:</p>
<ul>
<li><strong>Interactive:</strong> Specifies if a dialog box should open.</li>
<li><strong>Literals:</strong> Specifies how literals are represented.</li>
<li><strong>Allow Check:</strong> Allows table rights to be checked.</li>
<li><strong>Allow Cross Company:</strong> Allows the query to include more than one legal entity.</li>
<li><strong>Form:</strong> Provides a form for user interaction with the query.</li>
<li><strong>Query Type:</strong> Join or Union.</li>
<li><strong>Searchable:</strong> If the query results are searchable.</li>
<li><strong>User Update:</strong> If the user can update the query.</li>
</ul>
<p>Queries also have nodes like Methods, Events, and Data Sources.</p>
<ul>
<li><strong>Methods:</strong> Indicates the methods for the query.</li>
<li><strong>Events:</strong> Exists, but is not used.</li>
<li><strong>Data Sources:</strong> Indicates the data sources in the query.</li>
</ul>
<p>Data sources have a few key properties to note:</p>
<ul>
<li><strong>Enabled:</strong> If the data source is active.</li>
<li><strong>Update</strong>: Data that is selected for update.</li>
<li><strong>Dynamic Fields</strong>: Specifies how a field list is built.</li>
</ul>
<p>Data sources also have a few key nodes:</p>
<ul>
<li><strong>Fields</strong>: The fields from the data source.</li>
<li><strong>Ranges</strong>: The filter for the data.</li>
<li><strong>Data sources</strong>: Indicates a data source can join to another data source.</li>
<li><strong>Group by</strong>: Groups the returned data.</li>
<li><strong>Having</strong>: Filter aggregate data.</li>
<li><strong>Order by</strong>: Order returned data.</li>
</ul>
<p>You can create or modify queries from code by using the "Query", "QueryRun", "QueryBuildDataSource", and "QueryBuildRange" classes. The document provides an example of how to use these classes to modify a query and add a range. Additionally, you can use the SysDa framework to create extensible queries without a user interface.</p>
<p>You can drag queries from the Application Explorer or Solution Explorer to use them as data sources or select them in a data source's query property. You must also build a query before using it or its extension. You extend queries by right-clicking on the query in Application Explorer, and then selecting Create Extension. When you are extending queries, you can add new data sources, fields, groupings, orderings, and having clauses. You cannot remove existing data sources, fields, groupings, orderings, or having clauses. If you set the Dynamic Fields property to Yes on the data source, then the system adds new fields automatically to the query, so you do not need to create an extension for new fields.</p>
<p>Lastly, the document covers <em>table maps</em>. Table maps are used to define fields and methods that operate on multiple record types and to create commonalities between similar record types. They prevent incompatible record assignments and simplify business logic. Table maps can be added to a project by right-clicking in Solution Explorer and selecting Add &gt; New Item. The table map designer has a top-level node that includes the name, and the other nodes include Fields, Field groups, Mappings, Methods, and Events. You cannot create an extension for a table map. Instead, you must create mappings in the table to table maps, or you have to use an interface class for specific logic. Mappings are used to create a relationship between the table and map, and they are performed in the mapping node of the table.</p>
<p>To create a mapping, you must use an interface class. You create an abstract class that is prefixed with "Interface" using the table map's name. Then you need to create a class for each table that will be using the map that extends the abstract class that you created. You initiate the interface class by using methods in the map. The code sample shows examples of each class. By using an interface class, you can create code extensions (chain of command) for that interface. You must build a table map before you can use it.</p>
<p>Here’s a breakdown of the key concepts:</p>
<ul>
<li><strong>Extending Views:</strong> Understand how to extend them and what you can modify.</li>
<li><strong>Queries:</strong> Know how to create and modify them in Visual Studio and code, and understand how they are different from views.</li>
<li><strong>Table Maps:</strong> Know their purpose and how to implement them using an interface class.</li>
</ul>
<p>For the exam, focus on how these concepts are practically applied. You should be able to identify when to use a query, how to extend a query, and when to use table maps. The example code will also help to understand how these different elements are implemented.</p>
<p>Okay, let's tackle this final section. We'll focus on forms, form patterns, data sources, and form methods. It's all about how you design the user interface for Dynamics 365, so it’s essential to have a solid understanding of the key concepts.</p>
<p>First, the document highlights that finance and operations apps use an <em>HTML web client</em>, which is supported by most major browsers, as well as mobile apps. Forms are used to display data to the user, and they are created and managed in Visual Studio. The form designer also has additional panes. The left pane is for methods, events, and data sources, the right pane is for patterns and sub patterns, and the bottom pane shows a preview of how the form will look in the user interface.</p>
<p>The document goes on to explain <em>form patterns</em>. Patterns provide a consistent structure for forms, and it is recommended to use them. They guide you through the necessary steps to create a form with the correct elements, and they help validate the structure, and control usage. They also help make the UI responsive and compatible with updates. Form patterns can also be nested by applying subpatterns to a container control, such as a FastTab.</p>
<p>The document provides a list of common form patterns:</p>
<ul>
<li><strong>Simple List pattern:</strong> Use for entities with six or fewer fields and no parent/child relationships.</li>
<li><strong>Simple List and Details pattern:</strong> Use for entities with six or more fields.</li>
<li><strong>Simple Details pattern:</strong> Used to present a simple set of fields, typically in view mode.</li>
<li><strong>Master Details pattern:</strong> A primary way to enter data; it uses expandable FastTabs.</li>
<li><strong>Details Transaction pattern:</strong> Presents data in a header and a line view.</li>
<li><strong>List Page pattern:</strong> A way to browse records and take actions on a record.</li>
<li><strong>Table of Contents pattern:</strong> Used for setup configurations that need multiple forms.</li>
<li><strong>Workspace pattern:</strong> Create groupings of tasks and pages.</li>
<li><strong>Wizard pattern:</strong> Used to guide a user through an ordered series of tab pages.</li>
</ul>
<p>The document then discusses how to add a <em>data source</em> to a form. Data sources are what connect your form to the data. You can add data sources from the AOT (Application Object Tree), or the data source node. When dragging a table from AOT to the data source node, you can select any of the tables in the system, or a table extension, for the data source.</p>
<p>You can also add a data source directly from the Data Sources node by right clicking and selecting New Data Source. You then specify which table or view to use in the properties for the data source. Once you've added a data source, you can add a grid, fields, and groups to the form.</p>
<ul>
<li>A <strong>grid</strong> displays the data in a table format in the user interface. It is added by creating a new control from the Design node.</li>
<li><strong>Fields</strong> are dragged from the data source on the left pane to the Grid node on the right pane.</li>
<li><strong>Groups</strong> are a way to add filters to the form, and can use a subpattern.</li>
</ul>
<p>The document then discusses <em>form methods</em>. Like tables, forms also have a set of standard methods.</p>
<p>Here are some key standard form methods to understand:</p>
<ul>
<li><strong>"init()"</strong>: Called when a form is opened. You can initialize variables, get parameters from args(), initialize data source queries, and set properties of controls in the "init()" method. Remember, modifications to the data source should only occur after the super() call because that is where the base initialization happens.</li>
<li><strong>"close()"</strong>: Called when a form is closed.</li>
<li><strong>"closeCancel()"</strong>: Called when a form is canceled.</li>
<li><strong>"closeOK()"</strong>: Called when a form is closed by selecting the OK button.</li>
<li><strong>"run()"</strong>: Called immediately after "init()"; it is used to display the form on the screen and populate the data.</li>
</ul>
<p>The document also provides descriptions of some common standard data source methods:</p>
<ul>
<li><strong>"init()"</strong>: Called when a form is opened after the form "init()" method. It helps create the data source query.</li>
<li><strong>"active()"</strong>: Called when a user navigates to a new record. It can be used to modify the UI based on the record.</li>
<li><strong>"validateWrite()"</strong>: Called when a user attempts to save a record. It can be used to validate the data. It is best practice to throw an error message rather than let the "validateWrite()" method fail silently.</li>
<li><strong>"validateDelete()"</strong>: Called when a user attempts to delete a record. It is used to confirm that a record is deleted. Like the "validateWrite" method, you should throw an error message rather than let the "validateDelete()" method fail silently.</li>
<li>"executeQuery()": Called when the query for the datasource is executed.</li>
<li>These methods (and all standard and custom methods) should always call the "super()" method to execute the base functionality. You can use chain of command to extend methods that are used in forms and data sources.</li>
</ul>
<p>Here's a quick summary of the key points:</p>
<ul>
<li><strong>HTML Web Client</strong>: The technology that finance and operations apps uses to display pages.</li>
<li><strong>Form Patterns</strong>: Understand their purpose, and why you need to use them.</li>
<li><strong>Data Sources</strong>: Understand how to connect data sources to the forms.</li>
<li><strong>Form Methods</strong>: Understand standard form methods such as "init()", "close()", "run()" and common data source methods such as "init()", "active()", "validateWrite()", "validateDelete()", and "executeQuery()".</li>
</ul>
<p>For the exam, focus on how these elements work together. You should be able to identify different form patterns and the different standard methods in forms. The examples provided in the text, and the purpose of each method, will help with understanding the different methods.</p>
<p>Okay, let's dive into this final section, focusing on form methods, menu items, testing form functionality, diagnosing performance, and optimizing forms. This is a crucial part of building a great user experience in Dynamics 365, so understanding these concepts is very important.</p>
<p>First, the document continues its discussion of <em>form data source methods</em>, with a focus on "executeQuery()", "initValue()", "write()", and "delete()". The document has already gone over "init()", "active()", "validateWrite()", and "validateDelete()". Remember, each form data source method will call the super() method to execute the parent functionality.</p>
<ul>
<li><strong>"executeQuery()"</strong>: This is called whenever a form is refreshed. It is used to filter data and display retrieved records on the form. This method differs from the "init()" method, because the "init()" method only runs when the form is opened.</li>
<li><strong>"initValue()"</strong>: Used to initialize values in the fields when inserting a new record.</li>
<li><strong>"write()"</strong>: Used to save the data back to the data source. You might use this method to update fields when a record is saved.</li>
<li><strong>"delete()"</strong>: Called when a record is deleted, and it can perform any additional actions that are necessary for a successful deletion.</li>
</ul>
<p>The document also discusses standard <em>form data source field methods</em>, including "modified()", "validate()", and "lookup()".</p>
<ul>
<li><strong>"modified()"</strong>: Called when a specific field's value is modified.</li>
<li><strong>"validate()"</strong>: Called when a value is entered into a field, and you can override it to add custom validation. Always provide an error message when a validation check fails.</li>
<li><strong>"lookup()"</strong>: Called when a lookup operation is attempted for a field.</li>
</ul>
<p>It is critical to remember that you can use the Chain of Command pattern to extend all of these methods. The code examples also show the correct code format for extending a form, a form data source, a form data source field, and a form control.</p>
<p>Next, the document discusses <em>menu items</em>. Menu items are how users navigate through the application. There are three types of menu items:</p>
<ul>
<li><strong>Display</strong>: Opens forms and dialog boxes.</li>
<li><strong>Output</strong>: Prints a result, such as a report.</li>
<li><strong>Action</strong>: Performs a job.</li>
</ul>
<p>Menu items make it easier for users to navigate to different parts of the application, without manually typing the web address into the browser. You can extend all three types of menu items, but you can only change the object for Action and Output menu items, not the Object type. This concept is important for the exam, as there are a few subtle differences in the different types.</p>
<p>The document then goes into creating and extending <em>menus</em>. Menus are the items that are located in the navigation pane of the finance and operations UI. You can create menus in Visual Studio, and you can create different components, such as menu item references, separators, sub menus, and tiles. The text provides a description of the different properties of a menu. You can move menu item references from the Solution Explorer or the Application Explorer to create menus quickly. You can also add workspaces to a menu by using a model extension in workspaces.</p>
<p>Next up is <em>testing form functionality and data connections</em>. It is a best practice to test your forms before they go into production. You can test a form by setting the form as a startup object in the Solution Explorer, building the form, and then previewing it in a browser. This will allow you to verify that the form pattern was applied correctly, and the correct fields are being displayed.</p>
<p>The document also goes into how to <em>diagnose and optimize client performance</em>. You can use various tools, such as the Performance timer tool, Fiddler, the Developer tools option in Chrome, and Trace Parser. These tools can help you diagnose a number of issues, including poorly performing forms, service authentication issues, slow loading times, and slow code. The document explains how to use each of these tools for analyzing performance and optimizing your form's performance.</p>
<p>The document explains how you can access the performance timer by adding debug=develop to the end of the URL, and how you can use it to separate the performance between the client and server.</p>
<p>The text provides steps on how to test your form's performance in a browser, by setting it as a startup object. You can also use the debugger in Visual Studio to run through the code, and the Infolog window to check the status messages.</p>
<p>The text emphasizes the importance of analyzing the data displayed on a form, and removing unnecessary fields to increase performance. It also provides a description of each of the tools that you can use to optimize client performance.</p>
<p>Here's a breakdown of what you should focus on:</p>
<ul>
<li><strong>Form Data Source Methods</strong>: Focus on "executeQuery()", "initValue()", "write()", and "delete()" methods.</li>
<li><strong>Form Data Source Field Methods</strong>: Understand "modified()", "validate()", and "lookup()".</li>
<li><strong>Chain of Command</strong>: Remember, chain of command can be used to extend all types of methods.</li>
<li><strong>Menu Items</strong>: Understand the differences between Display, Action, and Output menu items, and know how to modify them.</li>
<li><strong>Menus:</strong> Understand how menus are created and extended to display links in the navigation pane.</li>
<li><strong>Performance Testing:</strong> Know the purpose of the different tools, such as the Performance timer tool, Fiddler, the Developer tools option in Chrome, and Trace Parser.</li>
</ul>
<p>For the exam, focus on how these concepts are practically applied. You should understand the purpose of each tool and when it is appropriate to use it. You should also be able to identify the correct methods to use in different situations and how to implement a chain of command pattern.</p>
<p>Okay, let's dive into this final section, which focuses on classes, caching techniques, context classes, and how all of these work together to create customizations in Dynamics 365.</p>
<p>First, let's talk about <em>classes</em> in X++. Classes contain both the data (state) and methods (behavior) of objects. Think of classes as blueprints for creating objects. Some elements in Dynamics 365, like tables and forms, are built upon classes. The text states that classes can be used to manipulate data, such as inserting or deleting data from a table.</p>
<p>Classes can contain:</p>
<ul>
<li>Instance variables: bound within an instantiated object.</li>
<li>Static (or class) variables: bound to a class.</li>
<li>Methods: a sequence of statements that operate on the data in a class. Methods can be either instance or static.</li>
</ul>
<p>The text describes how to add both standard classes and runnable classes (or jobs) to your project in Visual Studio. Runnable classes contain a main method, and you can add methods to your class manually, including the "main()" method.</p>
<p>Next, the document emphasizes the importance of using <em>caching</em> to improve performance. Caching increases performance for data or code that is used often. There are three caching techniques to understand: SysGlobalCache, Singleton, and SysGlobalObjectCache.</p>
<ul>
<li><strong>SysGlobalCache</strong>: This is a cache that is specific to a single client session, and you can access it from system classes such as Application, ClassFactory, or Info. The SysGlobalCache class stores data on a specific key, and it has methods like construct, get, set, and isSet. You have to handle caching with care because it uses memory. It is best practice to remove the data when it is no longer needed.</li>
<li><strong>Singleton</strong>: A singleton class is another option that stores the values in memory for a single user session. A singleton is an instance of the class created within the class itself. You can see an example of the singleton pattern with flight classes in the standard application. The system creates a singleton for the user session.</li>
<li><strong>SysGlobalObjectCache</strong>: This is a server-shared cache, meaning that values are shared by all user sessions on a specific server. It is also a singleton, so it can only be used once by the system. The purpose of this cache is to store static values for all sessions, to avoid having to reread or recalculate them every time. You remove all entries by using the SysFlushAOD class in the browser. You access it through the URL.</li>
</ul>
<p>The key takeaway is that global caching stores data in memory to improve performance but must be handled with care. SysGlobalCache and singleton are for a single user session, whereas SysGlobalObjectCache is shared across all users.</p>
<p>The text then discusses <em>context classes</em>. Context classes are a way to add parameters to a method signature, because, with extensions, you cannot change the method signature. If the context class implements IDisposable, then you need to implement a dispose method for cleanup. This approach is useful when you need to pass extra parameters into a method.</p>
<p>Here’s a summary of the main points:</p>
<ul>
<li><strong>Classes</strong>: Understand their purpose, structure, and how to add them to a project.</li>
<li><strong>Caching Techniques</strong>: Know the difference between SysGlobalCache, singleton, and SysGlobalObjectCache.</li>
<li><strong>Context Classes</strong>: Know their purpose and when you should implement IDisposable.</li>
</ul>
<p>For the exam, focus on the practical application of these concepts. You should be able to identify when caching would be beneficial, and the different purposes of SysGlobalCache, singleton, and SysGlobalObjectCache. You also need to understand why context classes are necessary for extending existing method signatures.</p>
<p>Okay, let's wrap up this module. We'll focus on context classes, adding code to classes, data manipulation methods, and Business document management. It’s the final piece of our development journey.</p>
<p>First, the document picks up with <em>context classes</em>. As we discussed in the previous section, they are a way to add additional parameters to a method that you are extending, since you cannot change the method signature. Remember that if your context class implements the "IDisposable" interface, then you have to have a dispose method for cleanup. The example also shows a test runnable class which uses the dispose method to ensure that the data is cleaned up when you are done with the class. Using a singleton context class will allow you to transfer parameters without changing the method signature.</p>
<p>Next, let's talk about adding <em>code and methods</em> to classes. We already know that X++ is the programming language used for development in finance and operations apps and that it is object-oriented. The document lists some of the methods you will be using, including:</p>
<ul>
<li>"new()": The default constructor which is used to create an instance of an object by using the new keyword.</li>
<li>"finalize()": The destructor for a class, used to finalize an instance of the class by using the "finalize" keyword. It's often used in "if" statements to decide if the "finalize()" method should be called.</li>
<li>"main()":  A static, class method that is used when a class is run from a menu option. You can transfer data to the method by using the "_args" parameter.</li>
<li>"Instance Method": or object method, that must have a class instance created before it is called.</li>
<li>"Static Method": or class method, where you don't need to instantiate an object to use the method. It uses the static keyword.</li>
</ul>
<p>The document also lists the access modifiers that control access:</p>
<ul>
<li>"Public": Can be called from anywhere the class is accessible.</li>
<li>"Protected": Can only be called from methods in the same class, or a subclass of that class.</li>
<li>"Private": Can only be called from methods within the same class, and it cannot be overridden in a subclass.</li>
</ul>
<p>Remember to select the appropriate access keyword based on how you want the class to be used, and that these modifiers also impact class inheritance.</p>
<p>The document then discusses <em>data manipulation</em> with "select", "insert", "update", and "delete" methods. It explains that you use the "select" method to declare the data you want to modify, and you use a "where" clause with the "update" or "delete" method to define what data to modify. Also remember that you must use a "ttsbegin" and a "ttscommit" to create an atomic transaction. The document provides code samples for using each method. You can use "firstonly" to select only the first record, and you can use a "while select" method to loop through multiple records.</p>
<p>Finally, the document introduces <em>Business document management</em>. This feature allows business users to create and modify business documents (like invoices) through the web client. Users can also use Microsoft 365 or Microsoft Office to edit business documents, without relying on SSRS report development or source code changes. The tool uses the Electronic reporting framework to set up the documents, and it uses the Print management framework to link the documents to a printing option. Business document management supports output to Word, Excel, and PDF.</p>
<p>To set up Business document management, you need to configure parameters and attach a document type to SharePoint for the temporary storage of templates. Business users can then edit the templates from the Business document management workspace. Remember, the Business Document Type tag is used to determine which configuration is used for which output document, and this tag can then be linked to Print management.</p>
<p>You can also set up access permissions for the Business document management workspace, which restricts user access to specific templates.</p>
<p>Here's a summary of the main points:</p>
<ul>
<li><strong>Context Classes</strong>: How they are used to extend a method signature.</li>
<li><strong>Methods</strong>: Understand how to use the "new", "finalize", "main", instance, and static methods, as well as the access modifiers.</li>
<li><strong>Data Manipulation</strong>: Focus on how to use "select", "insert", "update", and "delete" statements.</li>
<li><strong>Business Document Management</strong>: Understand the purposes of this feature, and the different settings that need to be configured.</li>
</ul>
<p>For the exam, focus on the practical aspects of these concepts, and be able to identify the difference between context classes, and caching classes. You should also be able to identify the different methods and the situations when you would use each one.</p>
<p>Okay, let's tackle this final section on SSRS reports, query objects, Power BI, Excel reporting, and query optimization. This is all about how you get data out of Dynamics 365 and present it to users, so let’s make sure we understand the key concepts.</p>
<p>First, let's talk about <em>SSRS reports</em>. SSRS reports are used to print or view data from finance and operations apps, and they are great for high-volume transaction reports. They are used for documents that are printed, emailed, archived, or distributed in bulk. They can use an autodesign (for simple reports) or a precision designer (for more customized reports). You can add expressions for calculations, set fonts, add logos, and use labels. You can also schedule these reports by using batch jobs.</p>
<p>When creating SSRS reports, you first create a report object and assign a <em>data set</em>. The data set defines the data that is used in the report. You also have to select a <em>data source type</em>. These types are:</p>
<ul>
<li><strong>Query</strong>: Uses an existing AOT query.</li>
<li><strong>Business Logic</strong>: Used for a data source outside finance and operations, the name of the class must match the report name.</li>
<li><strong>Report Data Provider (RDP)</strong>: Used when you need additional logic.</li>
<li><strong>AX Enum Provider</strong>: Used to filter by an enum type.</li>
</ul>
<p>You can also <em>modify</em> existing SSRS reports by using extensions to add new columns to tables, or modify report data providers. The document explains that extending an SSRS report will not impact the standard reports, so you can use both.</p>
<p>Next, let's move on to <em>query objects and the Query Builder</em>. The text states that it is best practice to use queries instead of tables directly on forms, reports, and views because queries allow you to select specific fields to display. This reduces the SQL statement size and increases performance. The query object is used to create SQL dynamically at runtime.</p>
<p>The "QueryBuildDataSource" class defines the data source used in the query, and you can use it to define the table fields, sort order, and joins. The "QueryBuildRange" class defines filtering for the data source. The text gives examples of using both classes. There is also mention of advanced syntax that can be used for string, number, and date fields. This text provides examples of how to add a filter to include a particular enum and filter by values that are greater than a specific number.</p>
<p>You can also create queries that include related tables by using the different join types. The text provides an example of using the "ExistsJoin" to filter by a related table.</p>
<p>The document then transitions to <em>Power BI</em>. Power BI is a tool for exploring data, creating dashboards, and non-document reports. It uses OData endpoints to connect to data from finance and operations apps. You load data from entities, drag fields to the canvas, select visualizations, and then deploy the report. It emphasizes the ability to manipulate how the data is viewed easily. You can also get ready-made Power BI reports from Lifecycle Services and PowerBI.com.</p>
<p><em>Excel</em> is another tool that can be used for data manipulation. Finance and operations apps has an Excel Data Connector to help users view, update, and edit data. You can open data directly in excel by using the Open in Excel feature, which uses the AutoReport field group of the entity to determine which columns should be displayed. You can also filter the data. When you export data to Excel by using the Export to Excel feature, you export all the data in the grid, and you can then use the features of Excel to analyze the data.</p>
<p>The document then covers how to <em>optimize data source queries</em>. It states that you should only pull data that you need, and that unnecessary joins or additional fields increase the time for the report to run. You should also avoid using unique fields with a high cardinality in Power BI.</p>
<p>Finally, the document discusses using <em>groupings</em>, <em>expressions</em>, and labels* on an SSRS report. Groupings are used to organize the data into a meaningful layout, while expressions are used to retrieve, calculate, display, format, and perform aggregate functions. All text should use labels, so that they can be automatically translated. You can also use properties to control the font and margin sizes of the text and fields.</p>
<p>Here’s a summary of the main points:</p>
<ul>
<li><strong>SSRS Reports</strong>: Understand when to use them and how to create and modify them.</li>
<li><strong>Query Objects</strong>: Know how to use them to build SQL statements dynamically.</li>
<li><strong>Power BI</strong>: Understand its purpose and how to connect it with finance and operations apps data.</li>
<li><strong>Excel</strong>: Know how to use the Excel data connector.</li>
<li><strong>Query Optimization</strong>: Understand how to only pull data that is needed to increase performance.</li>
<li><strong>SSRS Report Features</strong>: Understand how to use groupings, expressions, and labels.</li>
</ul>
<p>For the exam, focus on the practical application of these concepts. You should be able to identify when to use each reporting tool, how to optimize queries, and how to format data correctly by using groupings, expressions, and labels.</p>
<p>Okay, let's wrap up this final section. We'll focus on workspaces, KPIs, drill-through elements, reusable RDL functions, built-in components, and data optimization. This is about how you provide users with a personalized and efficient experience within finance and operations apps, so let's make sure we understand the key concepts.</p>
<p>First, let's talk about <em>workspaces</em>. Workspaces are personalized dashboards that contain links, tiles, and lists to help users complete their roles. They act as a single point of entry to the application. Workspaces can be used to embed KPIs, and to manage day-to-day tasks for specific users.</p>
<p>Next up are <em>KPIs (Key Performance Indicators)</em>. These help users see how their business is performing. KPIs are created in the developer environment and embedded into workspaces. They use aggregate measurements, which are models that contain measures (like total sales) and dimensions (used to analyze the data), which is shown in the diagram. Aggregate measurements use the Default aggregate property to determine the aggregation, such as sum, min, max, count, etc. Aggregate measurements are deployed by using the Usage property, which you can set to:</p>
<ul>
<li>"InMemoryRealTime": Uses column store indexes in the SQL database for real-time data.</li>
<li>"StagedEntityStore": Uses the Entity store for near real-time Power BI reporting.</li>
</ul>
<p>The text walks through the steps to create aggregate measurements, aggregate dimensions, model dimension references, deploy aggregate measurements, create an aggregate data entity, and then create KPIs by using X++. KPIs can also be customized by the users.</p>
<p>Then the text talks about how to <em>add the KPI to a workspace</em>. This is done by creating a tile for the KPI and then adding a tile button to the workspace. The text emphasizes how to create the tile, and then how to create a form extension for the workspace to add the button.</p>
<p>Another important element for workspaces is the ability to create <em>drill-through elements</em>. Drill-through elements are links or buttons that allow you to navigate to more detailed data. The example that is given is that you might have a view of the total number of sales on a workspace, and then a link to see the details of each individual sale. You can add drill-through tiles by creating a query to pull the data, and a menu item that uses the query. The document walks through the steps of creating a tile and connecting the tile to the menu item. Then, the document shows you how to extend an existing workspace to add the tile to the summary section. You also have the ability to add a tile from the user interface with the Add to workspace feature.</p>
<p>The text then goes on to discuss the use of <em>RDL (Report Definition Language)</em> to create custom report functions. RDL is an XML representation of an SSRS report and can be used to define charts, graphs, calculations, text, and images. You can use RDL to create reusable report functions. You use the .Net Framework and XmlTextWriter classes to write RDL, and you can export reports as a .rdl file.</p>
<p>The document also discusses how to use <em>built-in components</em> to design workspaces, such as KPIs, charts, and other reporting components. Users can add existing elements to a workspace from the user interface. The presentation types are: Tile, List, and Link, and the Tile option can be used to configure the tile name and display the record count. The list option allows you to name the tab, and select the list style and columns that you want to display. The Link option creates a hyperlink to the workspace. Finally, the text also describes how you can embed Power BI reports in the workspace.</p>
<p>Here's a summary of the main points:</p>
<ul>
<li><strong>Workspaces</strong>: What are workspaces and how they are used.</li>
<li><strong>KPIs</strong>: How KPIs are created, and the relationship between aggregate measurements, dimensions, and aggregate data entities.</li>
<li><strong>Drill-through Elements</strong>: How to create these and link them to a menu item.</li>
<li><strong>RDL</strong>: Understand its purpose for defining report objects.</li>
<li><strong>Built-in Components</strong>: How to use them to add KPIs, charts, and other elements to a workspace.</li>
</ul>
<p>For the exam, focus on the purpose of workspaces, and the different options for implementing them. You should understand when to use a KPI versus a link, list, or tile. You should also be able to describe the use cases for RDL and Power BI.</p>
<p>Okay, let's tackle this final section on role-based security, Microsoft Entra ID, OAuth 2.0, and how to set up security in finance and operations apps. This is all about protecting data and ensuring that users have the right access to perform their jobs, so let’s make sure we understand these concepts.</p>
<p>First, let's talk about <em>role-based security</em>. In finance and operations apps, access is granted to security <em>roles</em>, not individual users. A user is assigned to one or more security roles based on their job responsibilities. Each role contains duties, privileges, and permissions that control access to system components. This role-based system is designed to align with the organizational structure. Users will have access to all of the duties that are granted to their assigned roles.</p>
<p>The key idea is that security is managed through a hierarchy:</p>
<ul>
<li><strong>Roles</strong>: Represent job functions (e.g., Accounts Payable Clerk, Sales Manager).</li>
<li><strong>Duties</strong>: Represent parts of a business process (e.g., Maintain vendor information).</li>
<li><strong>Privileges</strong>: Represent tasks (e.g., Generate a sales order).</li>
<li><strong>Permissions</strong>: Represent access to securable objects (e.g., menu items, tables).</li>
</ul>
<p>The document emphasizes that it is best practice to assign privileges to duties, and not directly to roles. You can also modify existing security elements by creating an extension. You can modify roles through the user interface, but modifications to duties and privileges must be made in Visual Studio.</p>
<p>The document also describes how to <em>create new security roles, duties, and privileges</em> in Visual Studio. You do this by adding a new item to your project in Solution Explorer. You can also assign a duty or a privilege to a role, or create extensions for these items. Additionally, the document emphasizes that permissions must be modified in the developer environment.</p>
<p>Next, the document moves into <em>permissions policies</em>. Permissions policies are used to grant access to users for specific purposes. They use a query from the AOT that filters data that should be returned. Field security profiles are created to restrict access to specific fields, and you can set up read, create, and write permissions. Permissions policies can control access to parts of the user interface, such as navigation pane pages and buttons on a page. The document also highlights different options for the Context Type, such as RoleName and ContextString.</p>
<p>The document also discusses the <em>extensible data security framework</em> (XDS), which is an evolution of record-level security (RLS) from previous versions of Dynamics AX. The XDS framework is used to restrict or deny access to specific tables, fields, and even rows in the database. It is always enforced regardless of the method by which a user is accessing the system. The XDS policy relies on a query that provides filters to the primary table and related tables.</p>
<p>The text explains that you can assign data security policies to roles and set the context of the security policy for a role or a group of roles.</p>
<p>Finally, the document touches upon how to <em>apply security permissions</em>. Users are assigned to security roles in the System administration module. You can also use queries from Visual Studio to automatically assign users to roles, or you can manually assign a user. You can also exclude specific users from automatic assignments.</p>
<p>The document also dives into <em>Microsoft Entra ID</em> and <em>OAuth 2.0</em>. Microsoft Entra ID is the primary authentication method for finance and operations apps. To access the system, users need a valid Entra ID account and must be assigned to a role.</p>
<p>OAuth 2.0 is used for integrations. It allows an application to access resources on behalf of a user. The document provides the steps to register finance and operations apps with OAuth 2.0 in Microsoft Entra ID, which includes creating a client secret and setting API permissions. You must also set up the finance and operations apps to work with Microsoft Entra ID by creating a record for the app in the System Administration Module. Remember that you need to copy the value from the secret because you won't be able to copy it later.</p>
<p>Here’s a quick summary of the main points:</p>
<ul>
<li><strong>Role-Based Security</strong>: Understand the hierarchy of roles, duties, privileges, and permissions.</li>
<li><strong>Security Elements</strong>: How to create, modify, and assign roles, duties, and privileges.</li>
<li><strong>Permissions Policies</strong>: How they are used to grant or restrict access to data and features.</li>
<li><strong>Extensible Data Security (XDS)</strong>: Understand how to control access to tables, fields, and rows.</li>
<li><strong>Microsoft Entra ID</strong>: The primary authentication method for the system.</li>
<li><strong>OAuth 2.0</strong>: How it is used for secure integrations with finance and operations apps.</li>
</ul>
<p>For the exam, focus on understanding the <em>why</em> behind these concepts. You should understand the relationship between roles, duties, privileges, and permissions, the purpose of XDS, and how to register an application using OAuth 2.0.</p>
<p>Okay, let's wrap up this final section, focusing on performance optimization in finance and operations apps. We'll look at caching, temporary tables, set-based operations, query optimization, diagnostic tools, and data migration. This is all about making sure your applications are fast and efficient, so let's dive in.</p>
<p>First, let's talk about <em>caching</em>. Caching stores data in memory, which can decrease loading times by reducing database calls. The document goes over two table caching options.</p>
<ul>
<li><strong>Set-based caching</strong>: Uses the EntireTable property and caches all records at once, but it should not be used for very large tables due to memory issues.</li>
<li><strong>Single-record caching</strong>: Caches one record at a time. It uses the "NotInTTS", "Found", or "FoundAndEmpty" property and uses both the client and the server cache.</li>
</ul>
<p>The document also discusses <em>display method caching</em>. Display methods can be cached by calling the "cacheAddMethod()" on a form, or by using the "[SysClientCacheDataMethodAttribute]" on the display method itself.</p>
<p>Next, let's discuss <em>temporary tables</em>. Temporary tables are used for efficient temporary data storage. There are two types:</p>
<ul>
<li><strong>InMemory</strong>: Uses an ISAM file on the client or AOS tier and stores data in memory until it reaches 128 KB. This type of table is used when you need to store data without writing to the database. The memory is only allocated while a record buffer exists.</li>
<li><strong>TempDB</strong>: Uses the SQL Server TempDB database. The data is removed when it is no longer needed by the current method, or the system is restarted. These types of tables can be joined to regular tables and can use indexes, and they also support joins, and are commonly used on reports to manipulate data.</li>
</ul>
<p>The text also lists some limitations of TempDB tables, such as the inability to manage date-effective data, delete actions, record-level security, and the fact that you cannot use them in views. The text also explains that you can use "linkPhysicalTableInstance" when linking data between TempDB tables, which is different from "setTmpData", which is used for InMemory tables. The text also states how to make a form use a TempDB or InMemory table by using the "setTmpData" method.</p>
<p>The text also discusses the different between <em>set-based and row-based</em> operations. Set-based operations manipulate multiple records at once using a single database call, while row-based operations manipulate one record at a time and need a database call for each operation. Examples of set-based operations are: "update_recordset", "delete_from", and "insert_recordset".</p>
<p>It is important to remember to use set-based queries whenever possible to increase performance because they make fewer calls to the database.</p>
<p>Then the document gets into <em>query optimization principles</em>. The following points are made:</p>
<ul>
<li>Queries should only pull the required data. Pulling all fields decreases performance.</li>
<li>You should avoid using nested queries, and you should use joins to combine the queries.</li>
<li>Use the appropriate join type (inner, outer, exists, or notexists) to avoid unneeded records.</li>
<li>Use indexes and index hints for large tables.</li>
</ul>
<p>The document also mentions different <em>diagnostic tools</em> to help identify areas for performance optimization, including:</p>
<ul>
<li>The Optimization advisor workspace in finance and operations apps.</li>
<li>Tools in Dynamics 365 Lifecycle Services, such as SQL Insights.</li>
<li>Trace Parser, SQL trace, and Performance SDK.</li>
</ul>
<p>Finally, there is the discussion on optimizing <em>data migration</em>. The document lists some configurations that can improve performance, including:</p>
<ul>
<li>Turn off change tracking.</li>
<li>Turn on set-based processing on the data entity.</li>
<li>Create a data migration batch group.</li>
<li>Enable priority-based batch scheduling.</li>
<li>Set up the maximum number of batch threads.</li>
<li>Import in batch mode.</li>
<li>Clean up staging tables.</li>
<li>Update statistics across tables.</li>
</ul>
<p>Here's a summary of the main points:</p>
<ul>
<li><strong>Table Caching</strong>: Understand how to set it up for improved performance.</li>
<li><strong>Display Method Caching</strong>: Know how to use "cacheAddMethod()" and "[SysClientCacheDataMethodAttribute]".</li>
<li><strong>Temporary Tables</strong>: Know the difference between InMemory and TempDB tables, and when to use them.</li>
<li><strong>Set-Based Statements</strong>: Understand how to use "update_recordset", "delete_from", and "insert_recordset" for bulk operations.</li>
<li><strong>Query Optimization</strong>: Know how to pull only the required data, avoid nested queries, and use the correct join type.</li>
<li><strong>Performance Diagnostic Tools</strong>: Know the tools that can help you identify performance issues and how they are accessed.</li>
<li><strong>Data Migration</strong>: Understand how to use the configurations in Data management to improve data migration performance.</li>
</ul>
<p>For the exam, focus on the practical application of these concepts. You should be able to identify when to use different types of caches or temporary tables. You should also be able to recognize when a set-based statement would be beneficial and be able to optimize queries.</p>
<p>Okay, let's wrap up this final section with a focus on variable scope, concurrency, parallel processing, asynchronous processing, Windows PowerShell, and some of the best practices related to these topics.</p>
<p>First, the document covers how to <em>modify the scope of a variable</em>. Remember that <em>instance variables</em> are declared in the class declaration and are available across the whole class instance (if they are not declared as private), while <em>local variables</em> are declared in a method and can only be accessed from within that method. The important point is that declaring local variables will use less memory than using instance variables. Therefore, you should try to use local variables whenever possible and declare them in the smallest scope possible to increase performance.</p>
<p>Next up is <em>concurrency</em>. There are two models to be aware of:</p>
<ul>
<li><strong>Pessimistic Concurrency Control (PCC)</strong> locks records when they are retrieved. This approach should be used when you need to serialize operations that require locks on the records, and when update conflicts are likely.</li>
<li><strong>Optimistic Concurrency Control (OCC)</strong> locks records when they are being updated. OCC is the model that is used by most tables in finance and operations apps because it has fewer resource requirements and keeps records locked for a shorter period of time. However, if an update fails, you will need a retry, which can lead to performance issues. You can set the concurrency model on the OccEnabled property, or you can override it in a Select statement.</li>
</ul>
<p>The document then describes <em>parallel processing algorithms</em> that you can use with batch processing. There are three common approaches:</p>
<ul>
<li><strong>Individual task modeling:</strong> This approach creates a separate task for each work item. It is good for small, interdependent workloads.</li>
<li><strong>Batch bundling</strong>: This approach uses a limited number of batch tasks with a bundle size. It is good for simple, even workloads with consistent processing times.</li>
<li><strong>Top picking</strong>: This approach creates a limited number of batch tasks, where each task processes the first free work item. It is good for uneven workloads with varying processing times, but using a staging table to store work items can affect performance.</li>
</ul>
<p>The document also explains the <em>Async Framework</em>, which is used for long-running processes that you don’t want to disrupt daily tasks. You must use the "runAsync" method, from either the FormRun or Global classes. If you are running a process on the client side, then it is best practice to use "FormRun", otherwise use the global class method for processes that run outside of the client. The method also needs a callback method that provides a result. It is important to also note the parameters that the methods take.</p>
<p>Finally, the document touches on <em>Windows PowerShell scripts</em>. Windows PowerShell scripts can be used to automate tasks in your development environment. You can use PowerShell scripts to perform actions such as database syncs, restarting services, resetting the data mart, and deploying models. When using PowerShell, you should always use error handling and logging, test scripts in a nonproduction environment, and follow security best practices.</p>
<p>Here's a summary of the main points:</p>
<ul>
<li><strong>Variable Scope</strong>: Understand the difference between instance and local variables, and how to use them to reduce memory usage.</li>
<li><strong>Concurrency</strong>: Know the difference between PCC and OCC, and when they should be used.</li>
<li><strong>Parallel Processing Algorithms</strong>: Understand the different options: individual task modeling, batch bundling, and top picking, and when they are appropriate.</li>
<li><strong>Async Framework</strong>: Understand how it is used to run processes in the background by using the "runAsync" method.</li>
<li><strong>Windows PowerShell</strong>: Know how it can be used to automate tasks.</li>
</ul>
<p>For the exam, focus on the practical application of these concepts. Be able to identify the appropriate use of PCC and OCC, as well as the different parallel processing algorithms. Also, understand the need for the async framework, and how to use it. Finally, know when using PowerShell is beneficial.</p>
<p>Okay, let's wrap up this entire learning path with a final, concise summary of key takeaways. This last part is about the importance of optimization and using the tools that are available.</p>
<p>The main idea here is that optimizing finance and operations apps is <em>critical</em> for system performance. Without optimization, users will experience slow load times and have difficulties completing their tasks.</p>
<p>Here are the key areas to focus on, which we've covered throughout the various units:</p>
<ul>
<li><strong>Data Retrieval</strong>: Pull only the necessary data by using proper queries, and avoid pulling unnecessary fields.</li>
<li><strong>Temporary Tables</strong>: Use InMemory tables for smaller amounts of data, and TempDB for larger amounts of data that require joins, indexes, or foreign keys. Remember that TempDB and InMemory tables are used for temporary storage.</li>
<li><strong>Concurrency</strong>: Use OCC where appropriate to increase throughput.</li>
<li><strong>Parallel Processing</strong>: Use individual task modeling, batch bundling, or top picking to handle a high number of tasks in batch jobs and asynchronous processes.</li>
<li><strong>Asynchronous Processes</strong>: Use the "runAsync" method to run long tasks without interfering with normal business processes.</li>
<li><strong>PowerShell Scripts</strong>: These can be used to automate frequent tasks.</li>
</ul>
<p>We've also gone over how to use the various diagnostic and optimization tools:</p>
<ul>
<li><strong>Optimization advisor workspace</strong>: This is where you will view a list of messages from the last performance check.</li>
<li><strong>Lifecycle Services</strong>: This tool is used to access tools such as SQL Insights to monitor performance.</li>
<li><strong>Trace Parser, SQL trace, and Performance SDK</strong>: These are tools to use in a development environment to determine bottlenecks.</li>
</ul>
<p>We also went over how to optimize data migrations using the data import/export framework by turning off change tracking, and using set-based processing with batch groups. Finally, remember that you should always test your work in a Tier-2 or higher environment.</p>
<p>In short, for your exam:</p>
<ul>
<li>Be aware of when to use which caching mechanism, or temporary table type.</li>
<li>Be familiar with how to write efficient queries using joins and using set-based operations instead of row-based operations.</li>
<li>Understand when to use asynchronous processes and the async framework.</li>
<li>Understand the importance of using diagnostics tools to analyze and improve performance, and the various tools available.</li>
</ul>
<p>By using all of these best practices, you’ll be able to create high-quality, high-performance solutions for finance and operations apps.</p>
<p>That's it – you've completed the entire learning path! Make sure to review the content and practice your skills. You've got this!</p>
<p>Okay, let's dive into this document about customizing Dynamics 365 Finance and Operations apps. It's a lot to take in, but don't worry, we can break it down and focus on the core ideas.</p>
<p>The big theme here is how Microsoft is pushing for a cleaner, more sustainable way to customize the system, and it's all about <strong>extensions</strong> rather than directly modifying the base code. Think of it like this: instead of ripping apart the engine of your car to add a new feature, you're using carefully designed add-ons that plug into the car's existing systems. This approach makes the car (your Dynamics 365 system) easier to maintain and upgrade.</p>
<p>Why is this so important? Well, Microsoft releases updates to Dynamics 365 every month. If you were to directly change the base code, each update would be a nightmare, requiring you to re-apply all your customizations, which is time-consuming, costly and prone to errors. Extensions, on the other hand, are designed to work smoothly with these updates. They live outside the core system, so Microsoft can make changes without breaking your customizations, making upgrades much easier.</p>
<p>The document explains the importance of not "overlaying." This is the old approach of directly modifying the standard code, and Microsoft is very clear that we should avoid it. It's like painting over a cracked wall – it might look good for a little while, but the cracks will come back, and they will be much harder to fix.</p>
<p>Now, let’s talk about <strong>how</strong> you actually create these extensions. You do it in Visual Studio, which is the main development environment for Dynamics 365. Most of the time you need to add references to the core Dynamics 365 models. Think of a model as a container for functionality. The most common models to reference are "ApplicationCommon", "ApplicationFoundation", "ApplicationPlatform", "ApplicationSuite", and "Directory". If you are having issues creating extensions and the "Create extension" is dimmed, you need to update the Model parameters and reference the required models. This is like selecting the right libraries to use when programming. The document also covers how to create a new model for customizations using "Create model wizard".</p>
<p>You can extend many elements in the application, like:</p>
<ul>
<li><strong>Labels:</strong> You can add new labels or modify existing ones to customize the user interface.</li>
<li><strong>Enumerations (Enums):</strong> If they are designed to be extensible, you can add new options to dropdown menus, for example.</li>
<li><strong>Extended Data Types (EDTs):</strong> You can change the characteristics of data fields, like their size or label.</li>
<li><strong>Tables:</strong> You can add new fields, indexes, or relationships between data.</li>
<li><strong>Data Entities:</strong> These are like simplified views of tables that can be used for integrations; you can extend these in much the same way you extend tables.</li>
<li><strong>Forms:</strong> You can add new fields or controls to screens or customize their behavior.</li>
<li><strong>Menus:</strong> You can add new options to menus, like the main navigation menu.</li>
</ul>
<p>But what if you need to do something that extensions can't handle? That's where <strong>extensibility requests</strong> come in. You submit a request through Lifecycle Services (LCS), the cloud portal for Dynamics 365, detailing your need. Microsoft engineers then look at it, and if it's reasonable and doesn't jeopardize system stability or upgrades, they might implement it into future versions, making it accessible through extensions.</p>
<p>The document goes on to talk about best practices which includes:</p>
<ul>
<li>Reference only necessary models.</li>
<li>Keep customizations modular and reusable.</li>
<li>Adhere to naming conventions.</li>
<li>Document customizations.</li>
</ul>
<p>Let’s go deeper into <strong>how</strong> extensions affect application functionality. There are a few ways to do this:</p>
<ul>
<li><strong>Class Extensions:</strong> These are used to add methods or modify the logic associated with a table or other component. You create a separate class, tag it with an "[ExtensionOf]" attribute, add the suffix "_Extension", and make the class final.</li>
<li><strong>Event Handlers:</strong> These are bits of code that run when something specific happens in the system, like when a record is deleted or changed. You can use these to extend behavior by responding to different events.</li>
<li><strong>Chain of Command (CoC):</strong> With CoC, you can wrap code around existing methods in the application. It's like adding layers to an onion; each layer (extension) gets to process the call. You call "next()" to execute the original code or any other extensions that may also be in the chain. The method must call "next()" unconditionally to ensure every method in the chain factors into the final result.</li>
<li><strong>Pre-event and Post-event handlers:</strong> You can use these to have code execute before or after an existing method runs, surrounding it like bookends.</li>
<li><strong>Attributes:</strong> You can apply attributes like "[Hookable(false)]", "[Wrappable(false)]" or "[Replaceable(false)]" to methods to prevent people from using pre-handlers, post-handlers, wrapping or CoC.</li>
</ul>
<p>Finally, the document talks about the <strong>SysOperationSandbox framework</strong>. This framework is designed to prevent the web client from timing out while running long processes, and is also helpful for preventing the UI from freezing. It allows the system to handle operations asynchronously. It's essentially a way to run resource-intensive tasks on a separate thread, keeping the user interface responsive. It is recommended to derive classes from "RunBaseBatch" so the state of the class can be serialized into the "pack()" and "unpack()" method.</p>
<p>So, to sum it all up, the document emphasizes a move towards using extensions as the primary means of customizing Dynamics 365 Finance and Operations apps. This approach helps maintain the integrity of the system while allowing for flexible customization. It is key to understanding the use cases of class extensions, event handlers, Chain of Command, pre- and post-event handlers, attributes, and how the "SysOperationSandbox" framework will avoid client disconnects during long running processes.</p>
<p>It's a lot to absorb, but the core message is this: Microsoft wants you to customize thoughtfully and in a way that doesn't break the system or future upgrades. So it is important to leverage all the extension capabilities available. Does that make sense?</p>
<p>Alright, let's break down this next section. It's got two main parts: using the sandbox framework and implementing workflows.</p>
<p>First up, we have the <strong>Sandbox Framework</strong> example. The key takeaway is how it allows you to run longer, asynchronous processes without freezing the user interface. This example shows how to insert 100 check numbers into a custom table.</p>
<p>Let's look at the code step by step.</p>
<p>We have a class "SandBoxLabHelper" with three methods:</p>
<ul>
<li>"AsyncInfo": This is a static method that contains all the logic. It takes a container of parameters ("_parameters") and uses it to create and insert "BankChequeTable" records. It takes a bank account, start check number and the count of check numbers as parameters. It loops through the check numbers and inserts them into the table.</li>
<li>"pack()": This method serializes any state that this class needs. Since we aren't serializing anything here, it just returns "conNull()".</li>
<li>"unpack()": This method takes the packed data and applies it to the state. In our case it does nothing and just returns "true".</li>
</ul>
<p>Then, we have a form with a button. The "clicked()" method of the button executes the "AsyncInfo" static method by using the "SysOperationSandbox" class, which does a lot behind the scenes:</p>
<ul>
<li>It creates a container ("mCon") and inserts the necessary parameters (bank account, start check number, and count of check numbers) into it.</li>
<li>It calls "SysOperationSandbox::callStaticMethod" to run the "AsyncInfo" method. This method call essentially starts a separate asynchronous process.</li>
<li>It provides labels for processing, completed and canceled status for the async process.</li>
</ul>
<p>The "SysOperationSandbox" framework is crucial here because it ensures the user interface remains responsive while the 100 new records are created in the database. Without this, the UI would freeze until the entire process was completed.</p>
<p>Now, let's move on to <strong>Workflows</strong>. The document describes how to create a simple workflow, and it's important to know the different components of a workflow and how they work together.</p>
<p>Here's the gist of what we're setting up:</p>
<ul>
<li>
<p>We are creating a new workflow type for records in the "MyWorkflowTable". Think of a workflow type as a template for processes.</p>
</li>
<li>
<p>We also have approval elements for the workflow to determine the different actions that can be taken on a record.</p>
</li>
<li>
<p>We want to allow users to submit a record for approval and change the status of the record based on the workflow state.</p>
</li>
</ul>
<p>The document walks through the process of creating a workflow step-by-step, which includes creating the following components:</p>
<ul>
<li><strong>Workflow Category:</strong> This just groups workflows by module for example Inventory, Project etc.</li>
<li><strong>Base Enum:</strong> We created an enum called "MyWorkflowStatus" that has the different states a workflow record can have, like "Submit", "Started", "Canceled", "Complete" etc.</li>
<li><strong>Table:</strong> We created a table called "MyWorkflowTable" that contains the data being used for the workflow process, and contains a field for the status of the record.</li>
<li><strong>Form:</strong> We created a form called "MyWorkflowTableForm" so the users can view and interact with the records in the "MyWorkflowTable".</li>
<li><strong>Display Menu Item:</strong> This menu item allows us to navigate to the form through the Dynamics 365 application. It is later added to the "InventoryManagement" menu so users can access it.</li>
<li><strong>Query:</strong> The query is used when creating the workflow type, it tells the workflow process which data source is being used.</li>
</ul>
<p>After creating the main components, the document then goes on to create the workflow type called "MyWorkflowType" using a wizard, which prompts you for the previously created workflow category, query, and display menu item.</p>
<p>The workflow type wizard generates the following classes and menu items:</p>
<ul>
<li>"MyWorkflowTypeSubmitManager": This class contains the logic for submitting the workflow, it creates a "WorkflowSubmitDialog" and gets the approval notes from the user and updates the status of the table to submit.</li>
<li>"MyWorkflowTypeEventHandler": This class contains the event handlers for when the workflow has started, been canceled or has completed.</li>
<li>"CancelMenuItem": This action menu item is used for canceling a workflow.</li>
<li>"SubmitMenuItem": This action menu item is used for submitting a workflow.</li>
</ul>
<p>A workflow event handler class is then created, where each event (started, canceled, completed) calls the static "updateWorkflowStatus" method on the "MyWorkflowTable" to update the status field.</p>
<p>The "canSubmitToWorkflow" method is overridden in "MyWorkflowTable", which specifies the conditions for when a record can be submitted to the workflow, which is when the status is not set to "Complete".</p>
<p>Next, the workflow form is configured so that the records in the form can participate in the workflow, by setting properties like "Title Data Source", "Workflow Data Source", "Workflow enabled" and "Workflow Type".</p>
<p>The document then guides you through the process of creating the <strong>workflow approval</strong>, which involves the following steps:</p>
<ul>
<li>
<p><strong>Document Type</strong>: The "MyWorkflowTypeDocument" class was created when creating the workflow type.</p>
</li>
<li>
<p><strong>Document Preview Field Group</strong>: A field group is created named "Workflow" on the "MyWorkflowTable" table, and the Num, Description, and MyWorkflowStatus fields are added to it. This defines the fields that are shown when the workflow is being approved.</p>
</li>
<li>
<p>The <strong>Document Menu Item</strong> from the form was already created.</p>
</li>
<li>
<p><strong>Workflow Approval Creation</strong>: A new workflow approval is created by using the Workflow Approval wizard. The wizard prompts you for the workflow document, document preview field group and document menu item that were previously created.</p>
</li>
</ul>
<p>The workflow approval event handler class "MyWorkflowEventHandler" is created which updates the status of the table when the workflow is started, canceled, completed, denied, change requested, and returned.</p>
<p>The labels of the different workflow approval menu items like "Approve", "Reject", "Delegate", "Request Change" and "Re-Submit" are set to the labels of the respective menu items.</p>
<p>The "MyWorkflowType" workflow type is updated so that the "Supported Elements" point to the newly created "MyWorkflowApproval" workflow approval.</p>
<p>Finally, the document explains how to test the created workflow in Dynamics 365, by activating the workflow type, configuring the basic workflow settings, and adding and connecting the workflow approval to the workflow process.</p>
<p>Phew, that was a lot! But it gives you a good idea about how you can create custom workflows in Dynamics 365.</p>
<p>To summarize:<br>
<em>   The <strong>Sandbox Framework</strong> is for running lengthy tasks asynchronously, without freezing the user interface.<br>
</em>   <strong>Workflows</strong> are about modeling business processes that involve steps, approvals and state changes to a record.</p>
<p>Do these explanations help clarify things for you?</p>
<p>Okay, let's recap the workflow and then get into the lab exercises.</p>
<p><strong>Workflow Recap</strong></p>
<p>The previous section detailed how to create a workflow in Dynamics 365. The section begins after the workflow type was created, which involved creating a category, enum, table, form, menu item, and query. Then the workflow type was created in Visual Studio with the Workflow Type wizard. Following that the document describes how to configure the workflow in the finance and operations application, which included:</p>
<ol>
<li>Configuring the basic settings.</li>
<li>Assigning the user who will approve the workflow. In this case, it was the Admin user.</li>
<li>Adding version notes, which is a best practice for tracking changes.</li>
<li>Activating the workflow, which makes it available for use.</li>
</ol>
<p>Once activated, the workflow becomes functional, and we should see new workflow actions on the form. So, that's the workflow setup and usage in a nutshell.</p>
<p><strong>Lab: Extending an EDT</strong></p>
<p>Now let's move on to the first lab exercise, which focuses on extending an Extended Data Type (EDT). The scenario involves modifying the "AgencyLocationCode" EDT to increase its string size from 12 to 14 digits. This is a good example of a non-intrusive customization. This lab is important because it highlights the following points:</p>
<ul>
<li><strong>Using Extensions for Simple Changes:</strong> Even seemingly small changes like adjusting the size of a field are done through extensions to maintain the integrity of the system.</li>
<li><strong>The Development Process:</strong> The lab covers the steps of creating a project, referencing models, extending an EDT, and making a minor property change.</li>
<li><strong>Importance of Referencing Models</strong>: Before creating extensions, it is important to reference the correct models so the application can find the base object that is being extended.</li>
</ul>
<p>Here are the steps that the lab covers:</p>
<ol>
<li>Ensure you are signed in to the lab with the correct credentials.</li>
<li>Open Visual Studio as an administrator, and start by opening it without code.</li>
<li>Reference the correct models so the application can find the base object that is being extended.</li>
<li>Create a new project named "FleetManagementExtensionProject" in Visual Studio.</li>
<li>Configure Visual Studio to organize projects by element type and synchronize databases on build.</li>
<li>Open the Application Explorer in Visual Studio and expand the AOT nodes for Data Types and Extended Data Types.</li>
<li>Search and select the "AgencyLocationCode" EDT.</li>
<li>Create a new extension for the "AgencyLocationCode" EDT by right-clicking the EDT. This creates an element called "AgencyLocationCode.FleetManagement" in your project.</li>
<li>Double click the new extension so it opens in the element designer.</li>
<li>Select the properties of the extension and change the string size from 12 to 14.</li>
<li>Finally, you perform a build of the project.</li>
</ol>
<p>The key here is that you're not altering the base "AgencyLocationCode" EDT directly. Instead, you're creating an extension of it and modifying the "String size" property in the extension, which is a very non-intrusive way of making the change.</p>
<p><strong>Lab: Extending a Form and Adding Controls</strong></p>
<p>The next lab exercise demonstrates how to extend a form and add a button to it, which allows users to navigate to a free text invoice form. This shows how extensions can enhance user interaction. The goal is to add a tab with a button to the "FMCustomer" form, which then links to a free text invoice page. This is also a great demonstration of non-intrusive customizations.</p>
<p>Here are the steps that the lab covers:</p>
<ol>
<li>Ensure you are signed in to the lab with the correct credentials.</li>
<li>Open Visual Studio as an administrator, and start by opening it without code.</li>
<li>Reference the correct models so the application can find the base object that is being extended.</li>
<li>Create a new project named "FleetManagementFormProject" in Visual Studio.</li>
<li>Configure Visual Studio to organize projects by element type and synchronize databases on build.</li>
<li>Open the Application Explorer in Visual Studio and expand the AOT nodes for User Interface and Forms.</li>
<li>Search and select the "FMCustomer" form.</li>
</ol>
<p>It appears that the document stops here, and does not provide the steps to create the extension and add the controls to the form. However, based on the instructions of the previous lab and the knowledge we have gained from previous sections, we can make the following assumptions:<br>
1.  Create an extension of the "FMCustomer" form by right clicking the form.<br>
2. Rename the extension with a company specific suffix.<br>
3. Open the form extension in the designer.<br>
4. Add a new Action Pane Tab to the form.<br>
5. Add a button group to the Action Pane Tab<br>
6. Add a menu item button to the button group and link it to the Free Text Invoice menu item.</p>
<p>After completing these steps, the user interface of the form will be changed and a new button will be available for the user.</p>
<p><strong>Quiz Questions</strong></p>
<p>Finally, let’s review the quiz questions:</p>
<ol>
<li>
<p><strong>"Creating extensions is the method used to make customizations in finance and operations apps. Which of the following is a benefit of using extensions instead of overlaying?"</strong></p>
<ul>
<li><strong>Correct Answer:</strong> Build times are reduced with extensions.
<ul>
<li><strong>Explanation:</strong> Extensions do not require recompilation of the entire application, while overlaying does.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>"Which of the following statements is true regarding extending enums in Visual Studio?"</strong></p>
<ul>
<li><strong>Correct Answer:</strong> Enums, where the IsExtensible property is set to True, can be extended.
<ul>
<li><strong>Explanation:</strong>  The "IsExtensible" property on an enum determines if you can add new values to it. If this is set to "false" then it can not be extended.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>"You want to create an extension from an element in the AOT. What must you verify to create this extension?"</strong></p>
<ul>
<li><strong>Correct Answer:</strong> Check that you're referencing the model that houses the element that you want to extend.
<ul>
<li><strong>Explanation:</strong> You need to have a reference to the model that contains the element that you want to extend; otherwise, the system will not be able to find it.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>"You create a new class by using a method. You need to protect the method from being extended by using Chain of Command (CoC). Which attribute or modifiers should you use to protect the method?"</strong></p>
<ul>
<li><strong>Correct Answer:</strong> Use "[Wrappable(false)]" as a method attribute.
<ul>
<li><strong>Explanation:</strong> The "[Wrappable(false)]" attribute prevents methods from being wrapped with CoC.</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>Summary</strong><br>
The main point here is that the preferred method of customizing Dynamics 365 is through extensions and not by modifying the base code directly.</p>
<ul>
<li>Extensions are non-intrusive and avoid conflicts that can occur with updates to the application.</li>
<li>Elements in the AOT like EDTs, enums, tables and forms can be extended.</li>
<li>Classes can be augmented by creating a new class with the "[ExtensionOf]" attribute, and it can contain methods and variables that are related to a table or form.</li>
<li>Event handlers can be used to modify the behavior of the application in response to specific events.</li>
<li>If an extension is not capable of performing the required change, then a request can be made to Microsoft through Lifecycle Services.</li>
</ul>
<p>So, we covered the workflow, went through the steps of extending an EDT and outlined how to extend a form, and answered the questions. How does all of this sound to you?</p>
<p>Alright, let's get through the rest of the material, starting with the conclusion of the form extension lab.</p>
<p><strong>Form Extension Lab Conclusion</strong></p>
<p>The lab finishes up by providing the steps to:</p>
<ol>
<li>Create the form extension from the "FMCustomer" form, which we discussed in the previous response.</li>
<li>Rename the form extension to "FMCustomer.Extension"</li>
<li>Open the form extension in the designer.</li>
<li>Add an Action Pane Tab, Button Group, and a Menu Item Button to the "ActionPane" control.</li>
<li>Set the "Menu Item Name" property of the Menu Item Button to "CustFreeInvoiceNew" which links to the free text invoice form.</li>
<li>Set the form extension as the startup object.</li>
<li>Build the project and then run the form.</li>
<li>The user can now view the new button on the "Invoicing" tab which will navigate them to the free text invoice page.</li>
</ol>
<p><strong>Menu Extensions</strong></p>
<p>After completing the form extension lab, the document covers <strong>menu extensions</strong>.  Menu extensions allow you to modify the navigation structure of Dynamics 365 without directly altering the base menus. This is great for adding your custom menu items or reorganizing existing ones.</p>
<p>Key things to know:</p>
<ul>
<li>You can create a menu extension from Application Explorer, just like with forms and other elements.</li>
<li>With menu extensions you can:
<ul>
<li>Add new items to existing menus.</li>
<li>Change the "Visible" and "Label" properties of menu items.</li>
<li>Hide existing items but cannot delete them completely.</li>
</ul>
</li>
<li>Menu extensions, just like other types of extensions, must be built to become active.</li>
</ul>
<p><strong>Delegates</strong></p>
<p>The next section is about <strong>Delegates</strong>. Delegates are a programming concept that allows you to define a point in the code where you can inject additional logic.</p>
<p>Key points about delegates:</p>
<ul>
<li>They are typically placed in the middle of a method if pre- and post-events or CoC are present.</li>
<li>They must have a void return type.</li>
<li>They can be static or instance methods.</li>
<li>Subscribers do not control the execution order.</li>
</ul>
<p>However, the document explicitly states: "We don’t recommend that you create new delegates in the application. Rather, we recommend that you subscribe to existing delegates and then use Chain of Command (CoC), which provides a richer, more robust, and more concise extension mechanism that supersedes delegates."</p>
<p><strong>Why Chain of Command over Delegates?</strong></p>
<p>The document emphasizes that CoC is a better alternative to delegates because it is more robust and provides a cleaner extension mechanism. With CoC, you can control the order of execution more predictably, as you are wrapping code around existing methods.</p>
<p>Delegates are generally used by Microsoft or ISVs to provide extension points in their solutions. So, developers should focus more on using the more modern extensibility features like CoC, event handlers, and class extensions.</p>
<p><strong>Quiz Questions</strong></p>
<p>Now, let's look at the quiz questions at the end of this section:</p>
<ol>
<li>
<p><strong>"Which of the following statements is true regarding table extensions?"</strong></p>
<ul>
<li><strong>Correct Answer:</strong> You can use an event handler to add business logic to a table.
<ul>
<li><strong>Explanation:</strong> Table extensions allow you to implement event handlers to customize the business logic of a table. Other options are incorrect because it is not possible to edit all properties in the properties window, you can create new indexes, and overlaying is not allowed.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>"You need to create an extension of the FMCustomer form. Which of the following tasks cannot be accomplished after you have created the form extension?"</strong></p>
<ul>
<li><strong>Correct Answer:</strong> Change the form pattern from Details Master to Table of Contents.
<ul>
<li><strong>Explanation:</strong> Form patterns cannot be changed with extensions.  Other options are incorrect because data sources and controls can be added and properties modified with a form extension.</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>Summary</strong><br>
To summarize, you can create extensions to modify the look, feel, behavior and functionality of different elements in the application. You can modify table fields, indexes and relations. Event handlers and class extensions allow you to modify the business logic of the table. Forms can be extended to add data sources, controls, and event handlers. Also, Menus can be extended to add or modify menu items.<br>
The preferred way to extend business logic is with class extensions, event handlers, and CoC, and delegates are no longer recommended.</p>
<p><strong>Business Events</strong></p>
<p>The final part of this material covers <strong>Business Events</strong>. These are mechanisms to notify external systems about significant occurrences in Dynamics 365, such as a customer invoice being posted or a sales order being confirmed.</p>
<p>Here's a breakdown:</p>
<ul>
<li><strong>What are Business Events?:</strong> Business events represent actions performed within the system, such as a workflow approval or a confirmation of a sales order. They allow you to trigger actions or integrations in external systems based on events in Dynamics 365.</li>
<li><strong>Business Event Catalog:</strong> The catalog in Dynamics 365 shows all available business events and the data that is included. You can access this from "System administration &gt; Setup &gt; Business events &gt; Business event catalog".</li>
<li><strong>Activation and Management</strong>:
<ul>
<li>Business events must be activated to be used.</li>
<li>They can be activated for all legal entities or specific ones.</li>
<li>Inactive events retain history for later use and can be activated later.</li>
<li>You can rebuild the business event catalog to ensure the most up-to-date list of events is available.</li>
</ul>
</li>
<li><strong>Parameters</strong>: You can adjust:
<ul>
<li>The retry count and wait time for delivery failures.</li>
<li>The number of endpoints per event.</li>
<li>Whether to use batch jobs for processing business events.</li>
<li>The caching time for key vault secrets used to authorize endpoints.</li>
</ul>
</li>
<li><strong>Performance Settings</strong>: You can adjust the number of processing threads and the bundle size to optimize performance.</li>
</ul>
<p><strong>Creating a New Business Event</strong></p>
<p>To create a new business event, you need a business event class and a business event contract class, The business event class extends the BusinessEventsBase class.</p>
<p>Steps to create a new business event class:</p>
<ol>
<li>The class should use a naming convention like "CustomerInvoicePostedBusinessEvent".</li>
<li>The class extends "BusinessEventsBase".</li>
<li>The "[BusinessEvents]" attribute is added to the class, and this attribute provides information about the event, such as the contract class, name, description, and module.</li>
<li>A "newFrom" static method is created to construct the class.</li>
<li>Private "parm" methods are created to maintain the internal state of the class.</li>
</ol>
<p>The document then begins to discuss the next steps but stops before completing the explanation, so we will leave it here for now.</p>
<p>Overall, we've covered a lot! We've gone through form extensions, menu extensions, delegates, and how they're being superseded by CoC, and we've started to explore the Business Events framework. Do you have any questions about anything before moving forward?</p>
<p>Okay, let's wrap up this module by covering the remaining information about Business Events and then summarize everything.</p>
<p><strong>Completing the Business Event Code</strong></p>
<p>Picking up where we left off, the document now explains how to complete the code for a business event.</p>
<p>First, we need to add the "buildContract" method to the "SalesInvoicePostedBusinessEvent" class. This method is crucial because it's responsible for creating the contract and populating it with the relevant data. This method should be decorated with "[Wrappable(true), Replaceable(true)]" attributes, which allows other extensions to wrap or replace the behavior of this method using Chain of Command. Also, this method is only called when the business event is enabled for a company.</p>
<p>"x++<br>
[Wrappable(true), Replaceable(true)]<br>
public BusinessEventsContract buildContract()<br>
{<br>
return<br>
SalesInvoicePostedBusinessEventContract::newFromCustInvoiceJour(custInvoiceJour);<br>
}<br>
"</p>
<p><strong>Business Events Contract Class</strong></p>
<p>Next, we need to define the "SalesInvoicePostedBusinessEventContract" class. This is the class that holds all the data that will be sent out with the business event. It extends "BusinessEventsContract" and should be decorated with the "[DataContract]" attribute.</p>
<p>The key components of the contract class are:</p>
<ul>
<li><strong>Private variables:</strong> These hold the data, such as invoice account, invoice ID, sales ID, dates, amounts, and the legal entity.</li>
<li><strong>"initialize" Method:</strong> This method sets the values of the private variables based on the data provided from the static constructor method.</li>
<li><strong>"newFromCustInvoiceJour" Method:</strong> This is a static constructor that creates an instance of the contract and initializes it with data from the "CustInvoiceJour" table.</li>
<li><strong>"parm" Methods:</strong> These methods act as accessors for the private variables and are decorated with the "[DataMember('')]" and "[BusinessEventsDataMember('')]" attributes. The "DataMember" attribute specifies the name of the data member, while the "BusinessEventsDataMember" attribute specifies the label for the data member.</li>
</ul>
<p><strong>Activating Business Events</strong></p>
<p>The document then explains how to activate the business events:</p>
<ul>
<li>You need to select the business event in the catalog and choose "Activate".</li>
<li>Business events can be activated for all legal entities or specific ones.</li>
<li>Active business events appear in the "Active events" tab.</li>
<li>Inactive events can be moved to the "Inactive events" tab and can be activated at a later time.</li>
</ul>
<p><strong>Business Event Errors</strong></p>
<p>The document also covers errors that can occur:</p>
<ul>
<li>Errors can occur when processing business events outbound to the configured endpoints.</li>
<li>The system retries to process the event several times, and if it is unsuccessful, it logs the error.</li>
<li>You can resend an error from the error logs.</li>
<li>You can download the payload for an error for offline processing.</li>
<li>If an endpoint is deleted, and a new one is created for a business event, the errors can be resent and they will now be processed by the new endpoint.</li>
</ul>
<p><strong>Role-Based Security for Business Events</strong></p>
<p>Then, we looked at role-based security for business events:</p>
<ul>
<li>Role based security for business events is disabled by default.</li>
<li>It needs to be enabled in the business event catalog.</li>
<li>Users can be restricted to only viewing and subscribing to events to which they have the appropriate security rights.</li>
<li>The privileges, duties and role-based access for business events are listed in a table.</li>
</ul>
<p><strong>Consuming Business Events</strong></p>
<p>The document talks about how different types of endpoints consume business events:</p>
<ul>
<li>Business events can be consumed by multiple different types of endpoints, like Azure Service Bus, Event Grid, Event Hubs, Blob Storage, HTTPS, Power Automate and Dataverse.</li>
<li>Endpoints can be set up by selecting and configuring the specific endpoint in Finance and Operations.</li>
<li>When creating a business event from Power Automate, the business event in finance and operations apps is active and an endpoint is created.</li>
<li>In Dataverse, an endpoint is created when a plug-in or an SDK step is registered on a finance and operations app business or data event.</li>
</ul>
<p><strong>Business Events and Power Automate</strong></p>
<p>The document provides details on how to use business events in Power Automate:</p>
<ul>
<li>You can use the "When a Business Event occurs" trigger.</li>
<li>You need to specify the instance, category, event, and legal entity.</li>
<li>You can parse the JSON data with a "Parse JSON" action.</li>
<li>You can download the schema from the Business Event Catalog.</li>
<li>You can use the parsed JSON to get a record from Finance and Operations by using the "Get a Record" action.</li>
<li>You can then send an email with the data from the record and the JSON by using the "Send an email" action.</li>
</ul>
<p><strong>Extending Existing Business Events</strong></p>
<p>The final topic is about extending existing business events. This involves adding new fields and logic. You can do this by creating a code extension for the business event contract class:</p>
<ol>
<li>Find the business event contract class in Visual Studio, for example "SalesInvoicePostedBusinessEventContract".</li>
<li>Create an extension of the class, for example "SalesInvoicePostedBusinessEventContract_MB500_Extension".</li>
<li>Add a new private variable.</li>
<li>Add a new "parm" method with the "DataMember" and "BusinessEventsDataMember" attributes.</li>
<li>Create a CoC for the "newFromCustInvoiceJour" method and call a new method like "myInitialize" to set the value of the new field.</li>
<li>Finally, you can verify the new data member and schema in the business event catalog.</li>
</ol>
<p>The document also explains how to update the Power Automate flow to include the extended field.</p>
<p><strong>Testing the Business Event Extension</strong></p>
<p>Finally, the document explains how to test the business event extension by generating an invoice from a sales order.</p>
<p><strong>Role-Based Security for Business Events (Again)</strong></p>
<p>The document provides more details on role based security for business events, and lists the privileges and duties that you can use to control access.</p>
<p><strong>Summary of the Entire Module</strong></p>
<p>Okay, let's zoom out and recap the big picture. This module was all about customizing Dynamics 365 Finance and Operations apps in a sustainable and non-intrusive way using extensions.</p>
<p>Here's what we covered:</p>
<ul>
<li><strong>Extensions over Overlaying</strong>: The core principle is to avoid modifying base code and instead use extensions.</li>
<li><strong>Types of Extensions</strong>:
<ul>
<li>EDT extensions for modifying data types.</li>
<li>Table extensions for adding fields, indexes, relations, and business logic.</li>
<li>Form extensions for customizing UI elements.</li>
<li>Menu extensions for modifying the navigation.</li>
<li>Class extensions for adding methods and event handlers.</li>
</ul>
</li>
<li><strong>Code Modification Techniques</strong>:
<ul>
<li>Event Handlers for reacting to events.</li>
<li>Chain of Command (CoC) for wrapping and extending method logic.</li>
<li>Delegates (with a recommendation to use CoC instead).</li>
</ul>
</li>
<li><strong>Asynchronous Processes</strong>: The SysOperationSandbox framework was explained to be used for long running processes without freezing the user interface.</li>
<li><strong>Workflows</strong>: The document covered the creation of a basic workflow, workflow types, workflow approvals, and their configuration.</li>
<li><strong>Business Events</strong>:  We learned about how to use business events to notify external systems of events occurring in Finance and Operations, and how to create, activate, manage, extend, and consume them using Power Automate.</li>
<li><strong>Role-Based Security</strong>: We went over the role-based security mechanisms for business events.</li>
</ul>
<p>We have gone through a lot, but by understanding these concepts, you'll be able to customize Dynamics 365 in a way that's maintainable, upgrade-friendly, and aligned with Microsoft's best practices.</p>
<p>Do you feel like this comprehensive overview helps solidify your understanding of this module? Let me know if you have any more questions or need any further clarification!</p>
<p>Alright, let's wrap things up by addressing the final quiz questions and going over the summary of business events.</p>
<p><strong>Quiz Questions</strong></p>
<p>Here are the quiz questions and their answers:</p>
<ol>
<li>
<p><strong>"What type of class supports constructing the business event, building the payload, and sending the business event?"</strong></p>
<ul>
<li><strong>Correct Answer:</strong> A business event class
<ul>
<li><strong>Explanation:</strong> The business event class, which extends "BusinessEventsBase", is responsible for constructing the event, creating the payload, and sending it. The contract class is only responsible for defining the structure of the payload.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>"To retain a history of errors and remove a specific business event from a process, what status should the business event be changed to?"</strong></p>
<ul>
<li><strong>Correct Answer:</strong> Inactive
<ul>
<li><strong>Explanation:</strong> Inactivating a business event prevents it from being processed while retaining the error history, which is helpful for auditing and debugging. Deleting a business event will also delete the error logs.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>"Which of the following business actions is a finance and operations user able to perform for their organization to handle daily tasks?"</strong></p>
<ul>
<li><strong>Correct Answer:</strong> Workflow or nonworkflow actions
<ul>
<li><strong>Explanation:</strong> Users in Dynamics 365 perform workflow actions, like approving a purchase requisition, or non-workflow actions, like confirming a purchase order. Both types of actions can generate business events.</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>Summary of Business Events</strong></p>
<p>Now, let's recap the key points about business events.</p>
<p>The summary emphasizes that you need to clearly understand <em>why</em> you're capturing a business event, that is you need to determine the reason for capturing the business event and how it will be used by the recipient.</p>
<p>Here are some guidelines:</p>
<ul>
<li><strong>Valid Use Cases</strong>:
<ul>
<li>Business events are designed to trigger actions in external systems in response to something happening in Dynamics 365, for example notifying a user or creating a record in another system.</li>
<li>The action in response to the business event should be to take a business action, not just transfer data.</li>
</ul>
</li>
<li><strong>Invalid Use Cases:</strong>
<ul>
<li>Business events are <strong>not</strong> meant for data export scenarios. For those you should use data management tools that are already available in finance and operations apps. If you need to transfer large amounts of data you should not use business events.</li>
</ul>
</li>
</ul>
<p>The document also lists some common use cases for business events, including areas like:</p>
<ul>
<li>Procurement</li>
<li>Reporting</li>
<li>Customer and Vendor Master Data</li>
<li>Month-End Close</li>
<li>Sales Quotations</li>
<li>Workflows</li>
<li>Batch Processing</li>
<li>Alerts</li>
<li>Case Management</li>
</ul>
<p>This list is not exhaustive and is meant to provide ideas and help with understanding business events.</p>
<p><strong>Final Thoughts</strong></p>
<p>This module has covered a lot of ground. We've delved deep into the world of extensions and how they are the preferred way to customize Dynamics 365 Finance and Operations apps. We also explored how to use business events to notify external systems of actions that happen in the system.</p>
<p>You have now gained a strong foundation in:</p>
<ul>
<li>The importance of using extensions over overlaying.</li>
<li>How to extend various elements, including tables, forms, EDTs, and menus.</li>
<li>The use of code modification techniques like event handlers and Chain of Command.</li>
<li>How to use the SysOperationSandbox framework to run long running tasks without freezing the UI.</li>
<li>The purpose of the workflow framework and how to use it.</li>
<li>The purpose of business events, their implementation, and how they can integrate with Power Automate.</li>
<li>Role-based security for business events.</li>
</ul>
<p>This knowledge will be invaluable as you continue to work with and customize Dynamics 365.</p>
<p>Do you feel like you have a solid grasp of the material now? Are there any other questions I can answer for you, or is there anything else you would like to go over?</p>
<p>Okay, let's unpack this whole data integration thing for Dynamics 365 Finance and Operations. It's a pretty meaty topic, but we can make sense of it together. Think of it like this: you've got your core finance and operations system, and then you've got all these other systems that need to talk to it – that's where data integration comes in.</p>
<p>The document lays out various ways these systems can "talk" to each other, focusing on different API (Application Programming Interface) choices. These APIs are basically the messenger between systems. It's like having different languages for communication; each API has its own rules and ways of transferring data.</p>
<p>The main APIs mentioned are:</p>
<ul>
<li>
<p><strong>OData:</strong> This is like a standard language for web-based interactions. It's good for when you need real-time updates because it lets you send requests and receive responses immediately. Imagine you want to see if you have a specific item in stock; OData can help you check instantly. It's built on standard web technologies, making it widely compatible and secure. Think of it as the "fluent" speaker, always ready to converse.</p>
</li>
<li>
<p><strong>Custom Services:</strong> These are essentially custom-built "messages" for specific business needs. If you need something very specific that a standard API doesn't cover, you can create a custom service. Think of it as a tailored suit, fitting precisely to your unique requirements.</p>
</li>
<li>
<p><strong>External Web Services:</strong> Sometimes, you might need to talk to services that exist completely outside the Dynamics 365 ecosystem. This is when external web services come into play. It's like calling a business on the phone to get things done.</p>
</li>
<li>
<p><strong>Microsoft Office Integrations:</strong> Sometimes you just need to work with data in tools you're already using, like Excel. These integrations make it easy to pull data into Office and work with it from familiar interfaces. It's like using a familiar notepad to jot down important information before processing it further.</p>
</li>
<li>
<p><strong>Recurring Integrations:</strong> This is more of a scheduled approach. Instead of real-time, you set up jobs to move data periodically – daily, hourly, whatever works best. It's like having a delivery truck that picks up goods and delivers them regularly. These can handle both file-based and data package formats.</p>
</li>
<li>
<p><strong>Data Management Package API:</strong> This is another file-based approach, but the scheduling isn't done within Dynamics 365 itself. It's like a different type of delivery truck that has to be scheduled from outside.</p>
</li>
</ul>
<p>One of the biggest points the document stresses is the difference between <strong>synchronous</strong> and <strong>asynchronous</strong> integrations. </p>
<ul>
<li>
<p><strong>Synchronous</strong> is like having a phone call. You ask a question and wait for an immediate answer. It's ideal when you need real-time feedback or confirmation that an action was taken. Think, for instance, of a customer placing an order online; the system needs to verify availability and update inventory immediately. OData is usually used for this kind of interaction.</p>
</li>
<li>
<p><strong>Asynchronous</strong>, on the other hand, is like sending an email. You send the message and don't need to wait for a reply right away. This is perfect for large amounts of data that don't need to be processed immediately, like a daily upload of hundreds of transactions. Batch data APIs are the stars of the show here.</p>
</li>
</ul>
<p>The document also gives some practical examples to really drive home the point. For example, a retailer using a third-party system for product management would benefit from <strong>synchronous</strong> OData integrations to keep product information updated in real-time within Dynamics 365. On the flip side, the daily transaction imports from a point-of-sale system are prime candidates for <strong>asynchronous</strong> batch data integrations.</p>
<p>The recurring integrations API and the data management framework are file-based. Data management framework is the only API supported for on-premise implementations. Recurring integrations are not. Recurring Integrations allow scheduling from within Dynamics 365 while Data management framework scheduling is external to the platform. </p>
<p>Another crucial point:  Dynamics 365, like any good system, has <strong>service protection limits</strong>. Think of it like a traffic controller. If an API is getting overwhelmed with too many requests, it will start throttling requests. This helps to keep the system stable and avoid a complete meltdown.</p>
<p>Then, the document dives a little into <em>data entities</em>, which are super important. Basically, a data entity is like a simplified view of data scattered across different places in your system. Think of it as an abstraction; it makes life a lot easier for developers because they don't have to dig through all the individual tables. It's like having a curated collection of data rather than a massive, unorganized library. They're also how you enable OData services, synchronous services, and asynchronous integrations.</p>
<p>You can also extend data entities – adding new fields, actions or change their behavior. This is super important because it lets you adapt the system to your specific requirements. You can create a new field on a table and add that field to your entity. You can also create a "code extension" that lets you tweak the entity's methods.</p>
<p>All of these APIs can be accessed via REST and SOAP protocols as well. The document points out that the type of API you select can also depend on the formats the data is in – whether it is a file or data package format. It also highlights the option to use Extensible Style sheet Language Transformation (XSLT) to transform data that is in XML format. </p>
<p>In a nutshell, this document is all about equipping you to make informed decisions about data integration when you're working with Dynamics 365. It's not a one-size-fits-all situation; you need to consider whether you need real-time updates or batch processing, and which API best fits your needs. By understanding synchronous and asynchronous patterns, different API choices, and data entities, you can build powerful and reliable integrations that keep your business running smoothly. It’s like learning the different tools in a toolbox to pick the correct one for the job.</p>
<p>Okay, let's keep this conversation going, diving into more details about how we manage data entities, custom services and external integrations in Dynamics 365 Finance and Operations.</p>
<p>First, the document emphasizes that when you're extending data entities, you really need to think about <strong>performance</strong>. It's not enough just to add a field; you have to be mindful of the impact on the system. This is where those performance considerations mentioned in a prior unit come into play. Things like making sure you're not adding too many fields that could slow down data processing. It is also ideal to have a naming standard for all of your new fields.</p>
<p>Now, let's talk about the <strong>staging table</strong>. Remember, when you enable your data entity for data management, a staging table is automatically created. It's like a temporary holding area for data during import/export. You can't extend it the same way you extend the data entity itself; you have to create a <em>table extension</em>. The key is to keep the fields and the data types on the staging table and the data entity extensions in sync. If you don't, you'll get an error when building your project. Think of it like having two lists of ingredients for the same dish; they need to have the exact same components.</p>
<p>After making changes, it's important to <strong>refresh the entity list</strong> in the finance and operations apps. This makes sure that any changes you made to the data entities, including the new fields you added, are available in the user interface. It's like reloading a webpage to see the new content after making edits. You can also see your changes in SQL Server by verifying the view and table created for your data entity and staging table.</p>
<p>Next, the document moves onto <strong>custom services</strong>, which are a great way to expose specific business logic to external systems. Imagine you want to do a special calculation; instead of trying to build it into the data entities, you can create a custom service. It's like having a custom-built calculator that performs a unique calculation for your specific needs.</p>
<p>When you create a custom service, it's always available on both <strong>SOAP and JSON endpoints</strong>. SOAP is an older, more structured way of doing web services, while JSON is more modern and common for web-based communications. With JSON services, your responses are always returned in the JSON format. The main things to consider when making a custom service include: request and response classes (with those special "DataContractAttribute" and "DataMemberAttribute" tags), and a service class with the actual logic. These are essentially the blueprints, the logic, and the communication tools used by your custom service.</p>
<p>The document gives a very clear example of creating a simple custom service that takes two numbers as input and returns their sum. This example shows all the pieces you need: the request and response data contract classes, the service class with the "summarize" method, and adding the method to the service and service group. This is a great way to see how the pieces all fit together. It even provides guidance on how to test the service using your preferred tools.</p>
<p>The document then talks about the <strong>Batch OData API</strong>.  Batch jobs are used to perform many tasks in Dynamics 365, but sometimes, those jobs fail. This is where the Batch OData API comes into play. You can now use it to monitor, restart failed jobs, or create automated flows using Power Automate when a batch job fails. It's about adding more flexibility and control. Think of it as an alarm system and automated restart feature for critical processes in your business. The document goes through the steps of making a Power Automate flow that restarts a failed batch job.</p>
<p>Next up, the document touches on <strong>exposing OData endpoints from data entities</strong>. OData is the go-to for web-based access to data, which is why you see it being mentioned so much. When you mark a data entity as "IsPublic", it becomes available through the OData endpoint. OData also provides you with create, read, update, and delete (CRUD) capabilities. The document points out that Insomnia is an awesome tool for interacting with OData services.  It's like having a specialized browser designed for interacting with API endpoints.</p>
<p>Lastly, the document talks about <strong>consuming external web services</strong>. This is how your Dynamics 365 system can connect to things happening outside the system. For instance, if you need to connect to a weather service to integrate weather data into your application. The main thing is to do this in a secure way, and that's where <strong>Azure Key Vault</strong> comes in. You use Key Vault to store your secrets, like API keys, instead of hardcoding them into your X++ code. It's like storing your house keys in a safe location, not under the doormat. </p>
<p>The document then provides code samples showing how to retrieve secrets from Azure Key Vault, create an HTTP client to call the external web service, and how to use JSON for requests and responses with data contracts. It's like the detailed recipe for connecting to an external service, ensuring you have the correct ingredients and tools to communicate effectively.</p>
<p>In a nutshell, this part of the document builds on the previous information, going into the details of how you can customize and extend your integrations with Dynamics 365. From the importance of staging tables to the benefits of custom services, Batch OData APIs and the secure consumption of external services, this document lays out the tools you can use to take your integrations to the next level. It’s about taking all the conceptual knowledge and getting into the details of execution.</p>
<p>Alright, let's wrap up this deep dive into Dynamics 365 Finance and Operations data integration by focusing on a few more key concepts, including integrating with Excel, Power Apps, and diving into some important details about data management and external web service integrations.</p>
<p>First, let's discuss how you can leverage <strong>Microsoft Excel</strong> with Dynamics 365. It's incredibly convenient, because often, users want to work with data using familiar tools. The Dynamics 365 add-in for Excel lets you open data from your data entities directly within Excel. You can do this either from within the Dynamics 365 interface by selecting "Open in Excel," or you can open Excel and access the add-in. Once the data is loaded into Excel, you can then view, update, and even add new records directly within the spreadsheet. After you have made your changes, you can publish those changes back to Finance and Operations. It's a really flexible way to work with your data, letting you make bulk changes without having to navigate back and forth between the Excel worksheet and the Dynamics 365 system. But remember, you can't create a new data entity using the Excel add-in, only view, update or add data to existing ones. It is like having a window into the database where you can make changes, but you cannot change the database structure itself.</p>
<p>Next up, let's delve into <strong>Microsoft Power Apps</strong> integration. Power Apps is an awesome tool that lets you build custom business apps without writing code. You can embed these apps directly within Dynamics 365 to extend the system's functionality. So, for example, let's say that your business needs specific data from an external system. You can create a Power App to pull data from that system and display it right inside your Dynamics 365 interface. This is fantastic because it allows you to integrate data from other sources and create specialized functionality, all from an intuitive interface. Think of Power Apps as lego bricks you can use to build your own custom interface for the system. You can access a Power App by selecting the Power Apps button on the Action Pane or display it as a new tab, FastTab, blade, or a new section within a workspace. As a developer, you have the ability to control which pages can embed a Power Apps app. </p>
<p>A critical thing to remember here is that <strong>both the Excel add-in and Power Apps use data entities</strong> to access your data within Dynamics 365. Data entities serve as the access points to the data. Additionally, you can use data entities that are set to "IsPublic" to create canvas and model-driven Power Apps. You can use the same data in Power Pages by leveraging the dataverse capabilities. This means data from Finance and Operations can now be used to create web applications.</p>
<p>Then, the document mentions <strong>Electronic Data Interchange (EDI)</strong> which is a very important topic when dealing with supply chain partners. With EDI, you can automate the exchange of business documents, such as invoices and purchase orders, between your company and its partners. This allows your business to reduce costly customizations. EDI solutions are available through Microsoft AppSource. It's like a standard language for business documents, making it easier for systems to talk to each other.</p>
<p>Next, let's talk about the <strong>Data validation checklist workspace</strong>. This workspace is designed to track data validation during new implementations, upgrades, or migrations. It's designed to keep your data processes on track. You can filter results by legal entity, area, or task. It also allows you to track which tasks are assigned to who and manage the overall process. It’s like having a project manager to keep your data migrations on track and ensure that your data is in great condition after migrations.</p>
<p>The document also covers troubleshooting service authentication issues by inspecting <strong>JSON Web Tokens (JWTs)</strong>. JWTs are commonly used to authenticate access to services. You can use tools like Fiddler to capture the JWTs and inspect the contents to make sure that all the necessary parameters are correct and verify authentication. JWT.io is another great tool that lets you decode and inspect the JWT. It is like having a security guard that checks each token used to access the system.</p>
<p>Now, let's dive into the <strong>Data management framework's package REST API</strong>. It's a RESTful API that allows you to integrate with Dynamics 365 by using data packages. This API is essential for importing and exporting data, and you can use it for both cloud and on-premises deployments. Keep in mind that on-premises uses Active Directory Federation Services (AD FS) for authentication while cloud deployments use OAuth 2.0. The great thing is that Microsoft has kept a single API set for both, making it easier to manage.  Also remember that recurring integration APIs are not supported for on-premises deployments.</p>
<p>The document gives detailed code examples using JSON for both <strong>importing and exporting</strong> data packages. To initiate an import or export process, you use the "ImportFromPackage" and "ExportToPackage" API methods respectively. For both imports and exports, you can use the "GetExecutionSummaryStatus" API method to check the status of the job. It's important to note that files remain in blob storage for a limited time, which is seven days.</p>
<p>Another topic touched on is <strong>change tracking</strong> for data entities. Change tracking allows you to perform incremental data exports instead of full exports. This is super important for efficiency. By enabling change tracking, you can significantly reduce the amount of data you export and the time it takes to process the information. You can choose to track changes to only the primary table, the entire entity, or use a custom query for more fine-grained control. Bring your own database (BYOD) allows for the tracking of deletions, while non-BYOD does not.</p>
<p>Lastly, the document discusses <strong>wrapper classes</strong> for consuming external web services. Wrapper classes provide a layer of abstraction around the actual code that is being used to connect with the external web service. This makes it easier to manage and modify the code without directly touching the underlying web service logic. It's like using a translation layer; the wrapper class can translate the requests and responses, allowing you to connect to different systems without having to rewrite all of your code.</p>
<p>In short, this final part of the document ties up all the different ways you can work with data in and out of Dynamics 365. From familiar interfaces like Excel, to the powerful customization of Power Apps, to the ability to handle large data imports/exports using the Data management framework, you have all the tools you need to build robust integrations. And finally, you have a variety of tools to track the processes and troubleshoot issues when they arise. It’s like having all the tools you need in your toolkit to accomplish almost any task.</p>
<p>Okay, let's dive into the final section of our data integration exploration, covering everything from wrapper classes to setting up recurring data jobs, and managing data transformations.</p>
<p>First up, let's talk a bit more about <strong>wrapper classes</strong>. We previously mentioned them as a way to make consuming external web services easier in Dynamics 365. Now, let's go into more details about how they work. You can wrap not just external web service interactions, but also methods within Dynamics 365 entities themselves. This means you can modify or add logic to existing methods in a controlled manner. Think of it like putting a new layer around an existing piece of code; you can change how it works without directly modifying the original code. The key thing to note is that when you create a wrapper, you need to use an extension class in X++. This class has to be marked as both "public" and "final".</p>
<p>The document also dives a bit more into how you create wrapper classes for connecting to external web services using <strong>C#</strong>. You start by creating a C# class library and adding a service reference to the SOAP web service you want to call. Then, in your class, you create methods that handle calling the external service. Finally, in your finance and operations project, you add a reference to your C# class library and then call your wrapper method from X++ code. It's like building a custom bridge that connects your Dynamics 365 instance to the web service. This bridge makes it very easy to call those external services by just calling the methods that you defined within the wrapper class.</p>
<p>Next, we have a few questions to verify your understanding of the material. Remember that when using the "GetExecutionSummaryStatus" for monitoring your API, the response messages will indicate a success, a failure, or that it is still processing. The status response will not be something like “Starting”. Change tracking is enabled from the <strong>Data management workspace</strong> on the data entity list page. Finally, importing APIs is indeed supported in both on-premises and cloud deployments; this statement is false.</p>
<p>Now, let's discuss <strong>setting up data projects and recurring data jobs</strong>. You can use the Data management workspace to set up recurring data jobs for imports and exports. For this to work, you can use either a file, or a data package. Keep in mind that files can be processed sequentially, which can be used for certain imports. Also, you need to register your application in Microsoft Entra ID to get the Application ID (client ID). It’s like getting a security key to use the Data management workspace.</p>
<p>The document goes through a step-by-step process for setting up a recurring data job for exporting data. You'll create an export job, specify the data entity, set up the schedule and the recurrence, and configure the authorization policy by adding your Application ID. It's all about automating your routine tasks and ensuring regular data synchronization. Some of the benefits of using recurring data jobs are automation, data consistency and reduction of manual errors. The execution log allows you to monitor job progress and resolve errors and issues when they arise. The exercise in this section really helps you practice what you have learned.</p>
<p>Next, the document explains how the <strong>OAuth 2.0 authorization</strong> flow works. OAuth 2.0 is the standard protocol used for authorizing access to web APIs. You'll register your application with Microsoft Entra ID, request an authorization code, and then redeem it for an access token to access resources. The document goes into detail of the entire process and mentions both the successful and the error responses. It's a standard way to authenticate and get authorization when dealing with web services.</p>
<p>Another key topic mentioned is <strong>monitoring the status and availability of entities</strong>. A data entity's availability is based on the configuration keys of the data entity itself and all the tables that are included. If any of these configuration keys are disabled, it could impact whether or not the data entity or portions of the data entity are available. You should always remember to refresh the entity list after making changes to configuration keys to make sure that the metadata is updated to reflect the latest changes. This refresh ensures that you're working with the most up-to-date information. The document also discusses how to use parallel processing to improve import times.</p>
<p>The document also highlights how you should use the <strong>job history clean-up</strong> functionality to keep the system running efficiently. You can use system batch jobs to automatically schedule the cleanup of job history. These records are archived into blob storage, where they are available for seven days, which helps to keep the performance of the system high.</p>
<p>Next up, the document touches on <strong>monitoring the run status</strong> of data import and export jobs. Jobs can have statuses such as "Not run", "Executing", "Succeeded", or "Failed". If a job fails, you need to review the execution log, infolog, or the staging data to determine the root cause of the error. This gives you the tools you need to resolve any issues you are seeing with the integration.</p>
<p>Finally, the document wraps up with <strong>data transformation</strong> topics. It mentions that finance and operations apps has the concept of staging tables, which you can use to transform the data before they are inserted into the destination table. You can do this transformation through configurations such as ignoring blank values, adding text qualifiers, or using enum labels.</p>
<p>In a nutshell, this module provides all of the foundational information to set up robust, and performant data integrations with Dynamics 365. You have learned the conceptual details of the different types of APIs and integrations, and have dived deeper into the implementation details. From working with familiar tools like Excel to using Microsoft Entra ID for authentication and setting up recurring data jobs, you've been equipped with all the fundamental building blocks for data integration. This information should assist in passing the certification exam.</p>
<p>Okay, let's wrap up our comprehensive exploration of data integration in Dynamics 365 Finance and Operations by covering a few final topics, including data transformations, Dataverse integration, and the use of Azure Data Lake.</p>
<p>First, let's focus on the <strong>data transformation</strong> options, specifically within the Data Management workspace. The document highlights several useful features you can configure, such as:</p>
<ul>
<li><strong>Auto-generated</strong>: This feature allows you to skip providing values for specific fields in your source data files because these values will be auto-generated by the system upon import. For instance, a party number.</li>
<li><strong>Auto default</strong>: This feature allows you to set a default value for a field during import, instead of using the value from the source file.</li>
<li><strong>Conversion</strong>: This feature lets you create value-to-value mappings, allowing you to convert values from your source to different values during the import process. Imagine a scenario where you need to convert boolean values to "Yes" and "No" values during the import.</li>
</ul>
<p>Additionally, you can perform data transformation between finance and operations apps and the Staging table. You can use virtual fields and computed columns to achieve this. Here are some critical points to remember for each:</p>
<ul>
<li><strong>Computed Columns:</strong> These are generated using SQL and computed at the SQL Server level.  They are mostly used for reads, and for writes you must write X++ code to parse the input value. The benefit is that, when possible, you should use these columns because they use SQL Server which makes them faster than using virtual fields.</li>
<li><strong>Virtual Fields:</strong> These fields are non-persisted; instead, they're controlled using custom X++ code. Both read and write operations happen through custom X++ code. These are best used when dealing with X++ values that can't be computed using SQL.</li>
</ul>
<p>Moving on, let's explore the <strong>Microsoft Dataverse integrations</strong>. Dataverse plays a crucial role in connecting finance and operations apps with Dynamics 365 Sales, Customer Service, and other customer engagement apps. It lets you achieve a One Dynamics 365 user experience, seamlessly synchronizing data across different applications and enabling digital feedback loops. You can set up your finance and operations apps to link to Dataverse when you deploy new environments via Lifecycle Services.</p>
<p>The document also mentions the <strong>Data Integrator for Admins</strong>, which uses pre-built templates with predefined entities and field mappings to synchronize data across Dynamics 365 applications. These templates provide a fantastic starting point for your integration projects, and provide the ability to transform the data before it is imported.</p>
<p>The document highlights that the preferred method for integration is the <strong>synchronous approach</strong>, which enables bi-directional data flow between finance and operations apps and Dataverse in nearly real time. However, an <strong>asynchronous approach</strong> is also available but it is one-directional and is not real-time.</p>
<p>You can also use <strong>virtual entities</strong> to integrate Dataverse with finance and operations apps. Virtual entities don't copy data, but access it directly. This means data only exists in finance and operations apps, which helps prevent data inconsistency. Dataverse can perform create, read, update and delete (CRUD) operations using virtual entities. Before you start creating virtual entities, you need to enable the Power Platform Integration feature.</p>
<p>You can also use <strong>dual-write</strong>, which does copy data, but it's worth noting that it’s designed to work in tandem with virtual entities and other complementary technologies.</p>
<p>The document provides details on how to enable the Power Platform Integration feature and enable virtual entities for Dataverse use. It also provides an example of how to add the Customers V3 entity to a Power App.</p>
<p>Now, let's explore <strong>composite data entities</strong>. These allow you to group multiple related entities together for a single export or import operation. Think of composite entities as a way to represent a complete document such as a sales order, or a vendor catalog. These are used for asynchronous integration rather than synchronous OData. They are supported by the data management platform and can be imported or exported using XML file formats.</p>
<p>Next up are <strong>aggregate data entities</strong>. These are conceptually different from standard data entities. Aggregate data entities are built on top of views to consolidate, summarize, and simplify data for analytical purposes. Aggregate data entities can be exposed using OData, which allows you to query them through a standard URL.</p>
<p>The document walks through the steps to create an aggregate measurement, dimension, and data entity, along with setting up all the necessary properties. It even provides sample output of the aggregate data entity.</p>
<p>Finally, the document introduces <strong>Azure Data Lake</strong> and <strong>Entity Store</strong>. Azure Data Lake Storage is a cost-effective way to store large amounts of data. It can help you leverage data for reporting, querying, and data transformation. Entity store, which is available in Azure Data Lake Storage, lets you perform advanced analytics and AI, including things like reasoning over data. Finance and operations apps can use Azure Data Lake to provide straightforward access to denormalized transactional data, in near real-time, which is very beneficial.</p>
<p>In summary, this module is a treasure trove of information that will help you become proficient at setting up and managing data integration in Dynamics 365. It not only covers the foundational aspects of the various types of integrations but also the critical details for performing data transformations and connecting to external sources. You have all the skills to make informed decisions about which integration method is best for your business scenario. This detailed information will be critical in passing the certification exam.</p>
<p>Alright, let's bring this journey to a close by covering the final topics related to data integration with Dynamics 365 Finance and Operations. This section focuses on making Entity Store available in Azure Data Lake, connecting to Azure Data Lake, change data, Power Platform convergence, and migration planning.</p>
<p>First, let's dive into making <strong>Entity Store available as a data lake</strong>. This is where you take your Entity Store data and make it accessible in Azure Data Lake for more robust analytics and reporting purposes. This setup lets you access denormalized data for analytics, and allows for rich data mash-ups in tools such as Power BI.</p>
<p>To enable this feature, the first step is to enable the <strong>automated Entity Store refresh</strong>. This will move the data into the Data Lake from Entity Store. This means you will no longer be using the Entity Store database for analytics. Keep in mind that this change is not reversible, so make sure you plan accordingly. After enabling auto-refresh you can specify the refresh recurrence. You can choose to refresh every hour, twice per day, once per day, or once per week. You can also manually refresh aggregate measures on demand.</p>
<p>Next, you'll need to set up your <strong>Data Lake integration</strong>. In system parameters, you will need to enable the Data Lake integration toggle. If you choose, you can enable the trickle update which allows small, frequent updates as data changes in the system. You'll need to provide the Application ID, secret, Key Vault DNS name, and Secret name for Azure Key Vault. Finally, you'll need to test the configurations before proceeding. With this setup, your Entity Store data will be populated in Azure Data Lake Storage, unlocking numerous possibilities for analytics and data science work.</p>
<p>Next, we explore the different ways of <strong>connecting to Azure Data Lake Storage</strong>. Using Azure Data Lake Storage provides many advantages, such as lower storage costs and the ability to use reverse pipelines for the export. The data will be stored as CSV files in Azure Storage. The standard export is not available in Tier-1 development environments. However, you can use a prototype from FastTrack on GitHub to enable this for Tier-1 environments, which is what the document focuses on in the next section.</p>
<p>The document also points to a solution on <strong>GitHub</strong> for connecting a cloud-hosted development environment to Azure Data Lake Storage using the CDM format. Unlike the standard export, the GitHub prototype will conduct a full export. To use this prototype you will need an Azure subscription, an Azure storage account, a Microsoft Azure Synapse Analytics workspace, connection to a SQL on-demand endpoint, access to Microsoft Azure Data Factory and Visual Studio.</p>
<p>After completing the necessary prerequisites, the process includes cloning the repository from GitHub, building the project, deploying it as an Azure Function, and enabling MSI files that are built. Additionally, you will need to configure your storage account, set up Azure Synapse Analytics, collect Azure Data Factory parameters, deploy the Azure Data Factory template from GitHub, connect your Azure Data Factory to your Finance and operations apps environment, and finally run your pipelines in Azure Data Factory.</p>
<p>Another valuable feature mentioned is the ability to capture <strong>change data in Azure Data Lake</strong> in near real-time. You can opt for this feature to capture data as it is inserted, updated and deleted. The system will also generate a separate Change feed folder. This means you can build real time data pipelines that react to changes in the system. It provides a history of table data changes that can be used for incremental updates in your pipelines. The document stresses the importance of following best practices such as using a watermark and orchestrating your downstream jobs.</p>
<p>Now, let's discuss <strong>Power Platform convergence</strong>. Microsoft Power Platform integration breaks down barriers for customers to use the full capabilities of the Power Platform suite with their finance and operations environments. This simplifies setup and unlocks features like dual-write and virtual entities. By enabling Power Platform Integration, business events will also converge in Dataverse, enabling the execution of C# plugins. Also, CUD (create/update/delete) events will be triggered in Dataverse as a result of OData operations within Finance and Operations. The Plugin Registration Tool will enable you to register C# plugins for business and CUD events from Finance and Operations. This will allow developers who work in Visual Studio to write, register, deploy and debug C# plugins for both Dynamics 365 apps, and Dataverse.</p>
<p>The document then provides a few questions to verify your understanding of the material. The Data Integrator templates include <strong>field mappings</strong>. To register your application to use OAuth 2.0, you must use the <strong>Microsoft Azure portal</strong>. <strong>Microsoft Dataverse</strong> is used to enable data flows between finance and operations apps and other Dynamics 365 applications. Data in Azure storage accounts are stored in <strong>CSV files, using the CDM format</strong> and finally, you must setup Microsoft Power Platform integration during deployment in <strong>Lifecycle Services, before deployment</strong> to your Azure-hosted environment.</p>
<p>The document then shifts the focus to <strong>data migration</strong> and provides you with key information for planning a migration, choosing your migration tools, and ensuring a successful transition. Before planning your migration, you should take note of what types of integrations tools are available and familiarize yourself with each one. This should assist in making informed decisions when you plan your migration.</p>
<p>The document also reminds you of integration patterns such as OData, Batch Data API, Custom Services, consuming external web services and Excel integrations.</p>
<p>Remember that <strong>OData</strong> provides a standard and uniform way to share data using RESTful web services. The <strong>batch data API</strong> works for situations where you need the Data management package API and/or recurring integrations. The package API uses OAuth 2.0 and is designed to support migrations with both cloud and on-premise deployments, while recurring integrations uses the Data Management framework and can exchange files between finance and operations apps and third party services using secure REST APIs and authorization.</p>
<p>To summarize, this final module gives you the foundational information for not only integrating your systems but also leveraging the available tools for data migrations. You should be well versed in all of the core concepts to help prepare you for the certification exam.</p>
<p>Okay, let's bring this data integration deep-dive to a final close with a focus on real-world migration scenarios, tools, and best practices for transitioning between systems.</p>
<p>First, let's revisit some key points about the different integration methods we've covered in this module. Remember, when you set up a recurring job, you'll need a Microsoft Entra ID <strong>application ID</strong> so that the application can interact with the job. Make sure to record this value.</p>
<p>The document also emphasizes that there is much planning and effort that goes into building integrations, and that it is important to understand the functionality and purpose of each method. Let's quickly recap the methods:</p>
<ul>
<li><strong>Custom Services:</strong> These can be created for specific business needs and are deployed on both SOAP and JSON endpoints. SOAP examples can be found on Github, as can JSON examples.</li>
<li><strong>Consume External Web Services:</strong> This allows you to bring web services into finance and operations apps by referencing a class library for authentication, requests, and responses.</li>
<li><strong>Excel Integration:</strong> This powerful tool lets users work with data directly in Excel using the Excel Data Connector add-in.</li>
</ul>
<p>The document also reminds us about the difference between <strong>synchronous</strong> and <strong>asynchronous</strong> patterns, where synchronous is a blocking request and response pattern and asynchronous is non-blocking. OData is the recommended method for synchronous patterns and Batch Data API is the recommended method for asynchronous patterns.</p>
<p>Now, let's look at some concrete <strong>migration scenarios</strong> and the tools that are best suited for them:</p>
<ul>
<li><strong>Scenario 1:</strong> Imagine an energy company with field workers who need real-time inventory updates while scheduling jobs using third party SaaS. This scenario calls for a <strong>custom service integration pattern</strong> to provide real-time inventory data on demand using a SOAP or REST endpoint.</li>
<li><strong>Scenario 2:</strong> Think of a company that needs to import a high volume of sales orders from an on-premises system periodically. This scenario is best implemented using a <strong>batch data API integration pattern</strong>, due to the high data volumes and the lack of a need for real-time updates.</li>
</ul>
<p>In summary, finance and operations apps offers numerous tools for data migration. The <strong>Data Management workspace</strong> provides the ability to define data projects and move data. <strong>Office integration</strong> lets users directly work with data via the Excel Data Connector. The document also mentions the <strong>Excel Workbook Designer</strong> which is used to design data that is being exported into Excel workbooks.</p>
<p>Next, let's cover the <strong>Bring Your Own Database (BYOD)</strong> feature. This allows administrators to export data entities from finance and operations apps into their own Azure SQL database. You can choose to do either a full push of all records or an incremental push of changed or deleted records. This is the recommended approach if you need to connect other analytic tools using T-SQL. The document also mentions that the option to create clustered columnstore indexes (CCIs) is enabled by default, which improves the performance of read queries. When using BYOD, you should avoid reading directly from staging tables. Instead, use SQL triggers to determine when the data is synced, and then you can hydrate the downstream reporting systems.</p>
<p>The document also emphasizes how important it is to <strong>test a data migration and validate output</strong>. This process starts with identifying relevant legacy systems and common static data, such as countries, states, and postal codes, that are often used between industries. Creating test plans is essential in every migration process, and you need to make sure that the right system permissions are applied, all the necessary backups are performed, and you always have a solid approach for verifying your data. The general approach for data migration includes extracting data from the source system, transforming the data, loading the data, and monitoring the data during and after the process.</p>
<p>Also, it is best to test in smaller pieces and compare the migrated data to the source system data. For example, if migrating a chart of accounts, migrate one period and compare against the source system.</p>
<p>The document also highlights the need to <strong>identify and extract source data</strong> and to ensure that all relevant data is being extracted from the source system. It is also important to properly identify the relevant data entities, and always map the source data to the correct format for your target. In this process, you might need to perform data cleansing as well.</p>
<p>Finally, the document explains how to <strong>generate field mappings between source and target</strong> data structures, using the Data management workspace. You can choose different data source types for the migration, and, when you map data entities, the system automatically generates a mapping. The type of data and whether it is a required field are examples of metadata that is used. Also, you should ensure that your values are being populated correctly, and you can use mapping details to ignore blank values, select text identifiers, or use labels instead of values for enumerators.</p>
<p>It is also important to support the transition between existing and migrated systems. You should perform thorough data validation between the source system and your target, and have the appropriate stakeholders sign off on the transition. Make sure you have a proper process for comparing your data before you move on.</p>
<p>To wrap things up, this module has been a comprehensive guide to data integration in Dynamics 365 Finance and Operations. You are now equipped to make informed decisions, choose the correct tools, and implement sound strategies.</p>
<p>Here are some final questions to assess your knowledge: <strong>OData</strong> provides the broad integration between finance and operations apps and other products. The <strong>batch data API</strong> enables you to exchange documents between finance and operations apps and external applications. The <strong>OData</strong> service is how the Excel Data Connector app interacts with workbooks. The custom service integration pattern should be used for the example that is mentioned about the energy company and field workers. Lastly, <strong>recurring integration</strong> would be used to exchange documents on a daily basis, which is mentioned in the final question.</p>
<p>With this, you have completed your journey. Good luck with your studies and preparation for the certification exam!</p>
<p>Okay, let's bring this data integration journey to its final stop. In this segment, we'll look at connecting to Azure Machine Learning and also wrap up everything we've learned by reviewing key concepts.</p>
<p>First, let's talk about how you can <strong>consume data from Azure Machine Learning (ML) service</strong>. Azure ML allows you to use machine learning models and apply them to your business data. It can also be used to predict future outcomes, analyze trends, and get deeper insights from your data.</p>
<p>The document does not provide details on how you would consume the data from the Azure Machine Learning service; however, let’s think about the possibilities. You would first need to establish the connection between Azure Machine Learning and Dynamics 365, which will most likely involve using an API. Then, you would send your data from Dynamics 365 into Azure ML, or retrieve data from an Azure ML model that has already been built. The Azure ML results can then be used within your Dynamics 365 environment, using a variety of integration tools.</p>
<p>With that being said, let's recap the important things covered in this module:</p>
<ul>
<li>
<p>You can use the <strong>Entity Store</strong> for basic reporting, and data analysis. It's integrated into Power BI, which makes it ideal for reporting within Dynamics 365 Finance and Operations.</p>
</li>
<li>
<p>If you need more flexibility for your reporting and data analysis needs, or need batch integrations, the <strong>Bring Your Own Database (BYOD)</strong> feature is ideal. BYOD requires a separate Azure SQL database and lets you use T-SQL to query your data.</p>
</li>
<li>
<p>Both BYOD and Entity Store rely on <strong>custom data entities</strong>, and you should carefully design them for optimal performance. You should aim for simplicity and encapsulation, having one public contract that serves all integration endpoints. Your entities should also represent a holistic view of your business object.</p>
</li>
<li>
<p>When creating custom data sources for BYOD, you must configure the entity export, publish your schema, and then export the data. BYOD export also has timeouts that you need to be aware of. Keep in mind that BYOD cannot export composite entities or entities without unique keys when doing an incremental push.</p>
</li>
<li>
<p><strong>Logic Apps</strong> are an excellent way to automate business processes by connecting various apps and services. You can use Logic Apps to process orders, send notifications, or move data between different systems, whether in the cloud, on-premises, or both. The growing gallery of Logic Apps connectors makes it flexible to create a multitude of integrations.</p>
</li>
<li>
<p><strong>Power Platform</strong> integrations allow you to connect to Power Automate, Power BI Desktop, and Power Apps by using connectors or OData. You can also integrate <strong>Microsoft 365</strong> directly with finance and operations apps.</p>
</li>
<li>
<p>Finally, <strong>Logic Apps</strong> can be used to automate enterprise application integration (EAI), business-to-business (B2B), electronic data interchange (EDI), and various other business processes.</p>
</li>
</ul>
<p>The document provides great examples to help understand each type of integration. Remember, the first step is usually connecting to the service. Then, you need to create your workflows, which can involve triggers, actions, and logic.</p>
<p>To wrap this up, let's answer the quiz questions to verify your understanding of the material:</p>
<ul>
<li>You can create a data export for a custom data source in the <strong>Data Management workspace</strong>.</li>
<li>Most data entities can encapsulate multiple related tables, so they are <strong>not</strong> limited to just one table.</li>
</ul>
<p>That completes the learning path for the Data Integration solutions in Dynamics 365 Finance and Operations. It has been a long journey, but you now have the skills and knowledge to confidently use the core integration features and tools to achieve your business needs. It also should prepare you for your certification exam!</p>
<p>Okay, let's bring this comprehensive data integration journey to a satisfying conclusion. In this final segment, we'll tie up loose ends, focusing on a few key concepts and ensuring that you're well-prepared for your certification exam.</p>
<p>First, let's start by recognizing that <strong>finance and operations apps have a seamless integration with Azure Machine Learning</strong>. This allows you to use data from finance and operations apps to build predictive models for things like demand forecasting. However, you do have the option to manually disable this connection if needed.</p>
<p>The document also stresses the importance of registering external services with <strong>Microsoft Entra ID</strong> before they can connect to your finance and operations apps. This process involves registering an app in Microsoft Entra ID and then adding the app in finance and operations apps, using the Client ID and a service account user ID.</p>
<p>The document then offers practical advice on <strong>optimizing data entities for performance</strong>. By enabling set-based processing for non-composite entities, you can collect and move rows in groups. This also reduces the need for frequent communication with the database. Additionally, splitting input files can help finance and operations apps take advantage of multithreading.</p>
<p>Now, let’s dive into the quiz questions. Power BI doesn't have a direct connector, it connects through OData. Also, Logic Apps does not have the ability to update tables directly in Power BI. Finally, you are not required to register a service in <strong>Lifecycle Services</strong>. This is the location where your environments are hosted.</p>
<p>Now, let's turn our focus to the <strong>Microsoft Power Platform</strong> and how it enhances Dynamics 365 Finance and Operations through integration.</p>
<p><strong>Power Automate</strong> is the star of the show when it comes to automating workflows. You can create connections, use prebuilt templates, and create custom flows that connect to various systems and services. To connect to finance and operations apps in Power Automate, the data must be accessible via a public data entity.</p>
<p>Power Automate includes several powerful triggers that you can use, such as <em>When a business event occurs</em>, <em>Workflow</em>, <em>Alert rules</em>, <em>Instant flows</em> and <em>Scheduled flows</em>. The actions that you can use with finance and operations apps include actions such as: <em>Create record</em>, <em>List items present in a table</em>, <em>Delete record</em>, <em>Execute action</em>, <em>Get a record</em>, <em>Get a list of entities</em> and <em>Update a record</em>. You can use the Parse JSON to parse the response from finance and operations apps, which allows you to use data in further steps.</p>
<p>In addition to Power Automate, <strong>Power Apps</strong> is a great way to build custom apps that connect with your Dynamics 365 data. You can connect to your data by making use of public data entities and can then use Power Apps to create useful custom interfaces for your business. Also, you can embed Power Apps directly into finance and operations pages. In a similar way, you can also embed third-party apps within your environment as well, which allows you to integrate your custom built apps.</p>
<p>Finally, the document covers the <strong>Common Data Model (CDM) and Dataverse</strong>, which provide a shared data language for your business and analytical applications, including Dynamics 365 applications. With the CDM, you can use standard entities, extend the standard entities, or create custom entities.</p>
<p>Also, the document discusses <strong>Dual-write</strong>, which is a powerful integration method that lets you seamlessly connect data between model-driven apps in Dynamics 365 and finance and operations apps in near real time. It is bi-directional, so you can have data going to and from the applications. To configure Dual-write, you must link your finance and operations environment to Dataverse using the Dual-write wizard, and then enable the entity maps.</p>
<p>With that said, we have reached the final section of the learning path! You have now gone through the conceptual topics, all of the integration APIs, and all of the methods that can be used for data integrations in Dynamics 365 Finance and Operations. You have also been given practical advice and examples that can assist in using all of the different methods. Hopefully this has been helpful in preparing you for your certification exam!</p>
<p>Alright, let's put a bow on this data integration adventure with a final review and two last quiz questions.</p>
<p>First, remember, if you're having trouble locating a table when creating a Power App that connects to finance and operations apps, and your data entities are not visible, the most likely cause is that the <strong>IsPublic property on the data entity is set to No</strong>. This property must be set to "Yes" for the entity to be accessible to Power Apps.</p>
<p>Second, to configure dual-write in finance and operations apps, you must be an <strong>administrator of the Microsoft Dataverse environment</strong> to create the link between the applications. Also, dual-write is not configured in the feature management workspace, and the link is created in the finance and operations apps environment.</p>
<p>The document wraps up by reminding us that as a developer, you have a vast arsenal of tools to make development more efficient, especially when using Microsoft services like Power Apps and Power Automate. Also, the Common Data Model lets you bring data from multiple systems and applications together in a standardized way.</p>
<p>You have now reached the end of the journey! You have covered data integration strategies and implementation. You should be able to identify key concepts and understand the value of each option. You should also be prepared to use this knowledge on the certification exam.</p>
<p>Good luck with your studies!</p>
<p>Okay, let's chat about this whole data migration thing for Dynamics 365, especially with the Microsoft certification in mind. Forget about the nitty-gritty steps for now; we're going to focus on the big picture, the core ideas.</p>
<p>So, the first thing they're stressing is: <strong>data migration is a big deal, and you need to be prepared.</strong> It's not just about dumping data from one place to another; it's about ensuring the business keeps running smoothly and, ideally, costs stay down. You really gotta understand what you're moving, where it's coming from, and where it's going. That's the heart of this whole module.</p>
<p>Think of it like moving houses. You don't just throw everything into boxes randomly and hope for the best. You plan, you sort, you maybe even get rid of some stuff you don't need anymore. Data migration is the same. You need to understand the old "house" (your legacy system), what "furniture" (data) you have, and how it all fits into the new "house" (Dynamics 365).</p>
<p>Now, they dive into <strong>different ways to move data</strong> – the integration patterns. They're not going into technical details here, but they do lay out the main options:</p>
<ul>
<li><strong>OData:</strong> Think of this as a universal language for data. It's great for real-time, synchronous interactions - like when you need to get a quick update on something right away. Imagine checking if a product is in stock before a field worker takes an installation job.</li>
<li><strong>Batch Data API:</strong> This is your workhorse for larger volumes of data that don't need real-time updates. Think of moving thousands of sales orders every few minutes - this is the tool for that job. It's like packing all your boxes into a truck that makes regular trips.</li>
<li><strong>Custom Services:</strong>  Sometimes, you need something special. If the standard tools don't cut it, you can build your own service to do exactly what you need. It's like hiring a custom moving company with specific equipment.</li>
<li><strong>External Web Services:</strong> This is about connecting to other systems that aren't Dynamics 365. Imagine pulling in payment information from a payment gateway, or data from a CRM. You're building bridges between different buildings here.</li>
<li><strong>Excel Integration:</strong> This is the quick and easy way for users to work with Dynamics 365 data. Imagine exporting data to Excel, working with it, and then importing it back. Think of it as moving small amounts of items using a convenient trolley.</li>
</ul>
<p>They also mention the difference between <strong>synchronous and asynchronous processes</strong>. It’s like this: synchronous is when you make a phone call and wait on the line for a response; asynchronous is like sending an email – you send it and get on with your day, you’ll hear back later. OData works best with synchronous requests and Batch Data API for the other. The exam expects you to understand the difference and how it relates to integration methods.</p>
<p>The module then brings up <strong>Bring Your Own Database (BYOD)</strong>. This is important! It's not about <em>migrating</em> the system itself; rather, it's about letting <em>your</em> database become an extension of Dynamics 365. It's where you push data from Dynamics 365 for analytical purposes, use data for integrations with other systems, and more. It's a dedicated space, separate from Dynamics 365, to process data your way.</p>
<p>And that’s why they’re recommending BYOD for a data warehouse, for BI tools using T-SQL (like when you want access the data in a more direct way, using database queries, for analytics and reporting), or when you need to batch integrate with another system. This is another core concept to grasp. You connect the Dynamics 365 to your own database, so you are in control.</p>
<p>Next up: <strong>planning and testing.</strong> This section stresses the need to identify those "old houses" - the <strong>legacy systems</strong>, and that these systems might have unique formats.  They also emphasize that your data may need to be cleaned up before you migrate. You need to <strong>identify and import static data</strong> (like country codes) that don’t change often, and that it's important for all systems.</p>
<p>The module also tells you that you need a solid <strong>test plan</strong>. You can’t just move everything and cross your fingers, as things can and do go wrong. Backup is an absolute must - before any migration. You need to think about the whole process from data extraction to validation, and you need to have a way to monitor the migration process, to catch errors early.</p>
<p>You need to understand the <strong>entities and elements</strong> needed for your specific migration. Like if you are migrating all purchase orders, that also implies bringing over things like vendors, tax codes, etc. The most important thing you should remember is that business continuity is always a priority. So, that means planning around downtime, and communicating clearly with the customer.</p>
<p>Lastly, <strong>data mapping</strong>. This is about figuring out how the data fields from the old system correspond to the data fields in Dynamics 365. It’s like translating between two languages. And of course, they remind you that data can come from a variety of sources and formats.</p>
<p>So, in a nutshell, this module is all about understanding <em>why</em> data migration matters, the different <em>ways</em> you can do it, and the <em>planning</em> you need to do to ensure a successful outcome.  It's not about memorizing code or steps; it's about understanding the fundamental principles. This is how they will try to catch you on the Microsoft exams. Don't focus on the technical low level stuff that they have provided, rather, understand the higher level concept. Does that make sense?</p>
<p>Alright, let's break down this next chunk of information. This part really dives into the nuts and bolts of data management within Dynamics 365, building upon what we discussed earlier.</p>
<p>First, they’re showing you the <strong>field mapping</strong> process when you import or export data. It’s like having a translator that takes the fields from the source data and puts them in the right place in the target table. You see the source fields and their associated table fields in the target. It's not just about the names; you also have metadata, like data types and if a field is required, and other options.</p>
<p>This is where you get to fine-tune the process. For example, if your source data has a text value that doesn’t match the target field value, you can use a "text identifier" to handle it correctly. For enumeration fields, you can use the label instead of the underlying value. This kind of detail is important for the exam.</p>
<p>They then talk about <strong>supporting the transition</strong> between systems. Data validation is the key here. You need to ensure that what you've moved into Dynamics 365 is accurate and matches the legacy system, validating the data after migration, before and after. It's about making sure everything is as it should be and that your team is focusing on moving the data correctly.</p>
<p>Now, let’s tackle those quiz questions – this will solidify our understanding:</p>
<ol>
<li>
<p><strong>Which of the following options enables broad integration between finance and operations apps and other products, systems, and applications?</strong> The answer here is <strong>OData</strong>. OData is designed for interoperability and allows diverse systems to communicate using RESTful web services.</p>
</li>
<li>
<p><strong>Which of the following options enables the exchange of documents or files between finance and operations apps and any external application or service?</strong> This one's <strong>Batch data API</strong>. This is specifically for handling data exchange and recurring integrations.</p>
</li>
<li>
<p><strong>The Excel Data Connector app interacts with Microsoft Excel workbooks through which one of the following options?</strong> The correct answer is <strong>OData.</strong> Excel uses OData to interact with the entities.</p>
</li>
<li>
<p><strong>The energy company scenario...which integration pattern should you use?</strong> This should be <strong>Custom service</strong>. Since the scenario involves a real-time query of inventory, a custom service is best suited to handle the immediate, synchronous request.</p>
</li>
<li>
<p><strong>You need to enable an exchange of documents between finance and operations apps and an external application at the end of each day. Which of the following would you implement?</strong> This is <strong>Recurring integration</strong>. Recurring integrations are specifically designed for scheduled data exchanges.</p>
</li>
</ol>
<p>So, how did you do? This quiz gives a good idea of what the exam will be looking for – that <em>understanding</em> and choosing the right tool for a given situation.</p>
<p>Now, let's move on to the second part of the text. They're now introducing the <strong>Data Management Platform</strong> as a whole. Think of this as a toolbox for all things related to moving and managing data within Dynamics 365.</p>
<p>Here are the important concepts to understand:</p>
<ul>
<li><strong>Data Entities:</strong> These are like business objects or views of data, representing things like customers or vendors. They are designed to be easily understood, and you can re-use them across different integrations. So instead of working directly with tables, you work with these abstract views of data.</li>
<li><strong>Data Project:</strong> This is your blueprint for a specific import or export process. It defines the entities you're working with, the data format, and how the fields should be mapped. It's like a set of instructions for moving your data.</li>
<li><strong>Data Job:</strong> This is the actual execution of a data project. When you run an import or export, a data job is created. Think of this as like an instance of your project plan being put to work. Each job can have its own history.</li>
<li><strong>Job History:</strong> This keeps a record of every execution of a data job, including errors and processing times. It allows you to see past operations and track your progress, so you can review and address any issues.</li>
<li><strong>Data Package:</strong> A single, compressed file containing all elements of a data project. You can use this to move your projects between environments. It’s like having a portable folder with everything you need to copy a data setup.</li>
</ul>
<p>They also stress the <strong>benefits of the data management framework:</strong></p>
<ul>
<li><strong>Selective Entity Migration:</strong> Only migrate what you need.</li>
<li><strong>Error Handling:</strong> Skip bad records and continue with the good ones. This makes things faster and keeps things moving.</li>
<li><strong>Direct Data Transfer:</strong> Move data between instances without needing to go through Excel or XML.</li>
<li><strong>Scheduled Imports:</strong> Automate data transfers using the batch framework.</li>
</ul>
<p>They then dig into <strong>different data management scenarios:</strong></p>
<ul>
<li><strong>Data Migration:</strong> This is what we've been discussing - the initial, bulk move of data.</li>
<li><strong>Set up and copy configurations:</strong> This is for copying settings between companies or environments, so you don't have to redo everything from scratch.</li>
<li><strong>Integration:</strong> This covers everything from real-time service integrations to asynchronous processes using batch methods.</li>
</ul>
<p>They also go into the <strong>data flow process:</strong> You load source data to a central storage, it moves to staging, and then finally to target tables using data entities. You can control the sequence of this process using the entity sequence feature.</p>
<p>Then they show you that the data entity is an abstraction layer between tables and integrations. These data entities encapsulate business logic, help with imports and exports, and enable integration. They also note that data entities can be exposed as OData services, enabling synchronous integrations, and they are core to ALM and demo data scenarios.</p>
<p>The text also outlines different <strong>categories of entities:</strong></p>
<ul>
<li><strong>Parameters:</strong> Settings and configurations.</li>
<li><strong>Reference:</strong> Small, simple data sets like units of measure.</li>
<li><strong>Master:</strong> Key business data like customers and vendors.</li>
<li><strong>Document:</strong> Transactional data like sales orders.</li>
<li><strong>Transactions:</strong> Posted transactions such as invoices.</li>
</ul>
<p>Finally, they bring up <strong>configuration keys</strong>. These are used to turn certain functionality on and off in Dynamics 365. It's essential to understand the impact of these keys on the entities you're using. These keys can be assigned to data entities, their source tables, table fields, and data entity fields.</p>
<p>They also recap the types of integrations supported (OData for synchronous, data pipeline for asynchronous), and the <strong>value of the Data management workspace:</strong></p>
<p>The workspace allows to see the data projects and filter them, and view project details. You can use the various project types: Project, Configuration, Integration, or Other.</p>
<p>So, it's a lot to absorb, but it's all interconnected. You need to understand how data entities, projects, and jobs relate to each other, the different scenarios in which they are used, and how this all contributes to a smooth data management process. Think about the ‘why’ and ‘how’ of each concept, not just the definitions. This is key to acing the Microsoft certification exam! Does this detailed explanation make more sense?</p>
<p>Okay, this section focuses on the practical aspects of using the Data Management workspace and how you actually move data. It emphasizes sequencing, mapping, and the views in the workspace.</p>
<p>First, let's look at the <strong>Data Management Workspace</strong> itself. You can see the projects, load them, delete them, and download their definitions. The <strong>Job history</strong> gives you details about past runs, while you can use filters by the dates of the executions to view specific runs.</p>
<p>When you are working with <strong>export tasks</strong>, you can download the data itself by using the <strong>Download</strong> button.</p>
<p>There are three types of projects: <strong>Export</strong>, <strong>Import</strong>, and <strong>Copy into legal entity.</strong> And don't forget that the <strong>All projects</strong> filter is there to choose which type of project you are working with.</p>
<p>Now, <strong>sequencing data entities</strong>. This is critical for a successful import. They explain there are two types of sequencing:</p>
<ul>
<li><strong>Sequencing entities within a data package:</strong> When you add entities to a data project, they are automatically sequenced based on the order you added them. The first one becomes 1.1.1, the second 1.1.2, and so on. This sequence is important so that dependent entities are loaded in the correct order. You can adjust these sequences by going to the <strong>Entity sequence</strong> option within the Data project (only on the Standard view) by changing the execution unit and sequence, and moving entities up or down in the list. They illustrate this with an example of Sales tax codes and Sales Tax groups. The Sales Tax codes must be imported before the Sales tax groups. If you put other entities that are not dependent on those, you can set them to be on other execution units so that they can be loaded immediately without dependency. Finally they also mention an option to "Auto Sequence" the entities.</li>
<li><strong>Sequencing data package imports:</strong> This is about the order of <em>entire</em> packages, particularly in Lifecycle Services. This is crucial to address dependencies across modules. So for example, you must load your setup data before you load your transaction data.</li>
</ul>
<p>Next is the concept of <strong>mapping</strong>. Mapping of the fields is automatic but, of course, you can modify it if needed. They show you the <strong>View Map</strong> option, and the Mapping visualization and Mapping Details views. Required fields are marked with a red asterisk (*). You can remove mappings if a field is not required. They also show you how to edit the mapping from source to staging in the same way after an import.</p>
<p>They also discuss how to <strong>regenerate a map</strong> if you’ve changed an entity or if the mapping is incorrect. This forces the mapping to be refreshed.</p>
<p>And finally, you can also <strong>auto-generate data</strong> if a field is missing in your source. For example, the "Party number" in a customer import can be automatically generated upon import if you don't have this data on your file.</p>
<p>The document also compares the <strong>Standard and Enhanced Views</strong> for Data Management Projects:</p>
<ul>
<li><strong>Standard View</strong> - This view is more streamlined, and it's where you can manage the Entity sequences and also define the order of entity processing.</li>
<li><strong>Enhanced View</strong> - This view offers a more detailed interface with controls for more complex data migration projects. You can also load default templates from this view.</li>
</ul>
<p>They also emphasize using <strong>templates</strong> - there are default templates delivered for finance and operations apps. They advise using the Enhanced View to load them. You can also create your own templates to suit your business needs. They also mention the execution levels, from 10 to 22 are reserved for shared system entities; and level 25, for company-specific general ledger entities. The rest can be loaded in parallel in bands of dependencies. Each band is defined by its execution level. The entity sequence should be followed, but it’s also a guideline, not a requirement.</p>
<p>They touch on the importance of including the same entity in multiple templates (like payment terms), if you need that entity in either template. But also, a single entity cannot be present more than once in the project, or it will be replaced. This means that you can override templates, without changing them directly, by importing your own templates with the entities you want, and the system will replace that entity within the project.</p>
<p>They also show the availability of some large combined templates for "System and Shared", "Financials", and "Supply chain management".</p>
<p>Finally, they talk about <strong>change tracking for entities</strong>. If you turn this on, you can then do incremental exports, which means only the data that's changed since the last export is transferred. This is great for efficiency, and for BYOD purposes can be used to track deletes.</p>
<p>The last section focuses on <strong>data import and export jobs</strong>:</p>
<ul>
<li><strong>Staging tables</strong> are created for each entity, allowing you to check, clean, or convert data before moving it into Dynamics 365.</li>
<li>The process is to first <strong>create the import or export job</strong>, then:
<ul>
<li>Define the project category.</li>
<li>Select entities to import or export.</li>
<li>Set the data format.</li>
<li>Sequence the entities.</li>
<li>Decide whether to use staging.</li>
<li>Map source and target.</li>
<li>Set security.</li>
<li>Run the job.</li>
<li>Validate the history.</li>
<li>Clean staging tables.</li>
</ul>
</li>
</ul>
<p>They recommend choosing project categories to manage and filter related jobs. A source data format is composed of the type of the source, file format, row delimiter, and column delimiter. And that the entities should be sequenced to address functional dependencies.</p>
<p>They stress the need to <strong>verify the security</strong> of data jobs. You can restrict access by role or users and by legal entities. It also shows that Data jobs are global, which might be desired behavior, but can be changed using the applicable legal entities option, so each legal entity can have access to only their corresponding jobs.</p>
<p>Finally, you can <strong>clean up the staging tables</strong> by entity, job ID, or data project.</p>
<p>The video should help to understand the process visually.</p>
<p>So, this whole section is about taking the theoretical concepts and putting them into practice. It's about managing data movement within Dynamics 365, understanding the various components, and knowing how to use them effectively. This, of course, is a key part of what the certification will test. It really drills down into the workspace and the practicalities of running jobs. What do you think? Is this all becoming clearer?</p>
<p>Okay, let's wrap up this module with a look at the labs, database movement operations, cross-company data sharing, and finally, test case management.</p>
<p>First off, the <strong>lab instructions</strong> for signing in are very specific because you're working within a virtual environment. This is a good practice for the exam – you'll likely encounter simulations or scenarios where you'll have to follow specific login procedures. Remember, your personal credentials won't work.</p>
<p>The first lab, "Explore the Data Management Workspace," is really about getting familiar with the interface. They guide you through creating a new <strong>template</strong>, adding an <strong>entity</strong>, and then navigating the different options within the <strong>Data entities</strong> tile to view the entity structure and target fields. They also show how to use <strong>Entity model view</strong>, and <strong>Entity category view</strong>. They also guide you to explore the <strong>Data task automation</strong> and <strong>Configure data source</strong> tiles. This is all about building familiarity with the workspace so you can navigate it quickly.</p>
<p>The second lab, "Export data using the Data management workspace", guides you step-by-step to export an entity, in this case "All customers". You are walked through creating an <strong>export job</strong>, adding an <strong>entity</strong> to the job and exporting it to <strong>Excel</strong>. Then, the lab instructs you to view the generated file. This provides practical experience in how to actually move data out of Dynamics 365.</p>
<p>Now, let's get into the <strong>database movement operations</strong>. This is all about moving databases between different environments. The key thing to understand here is the different options available and when to use them:</p>
<ul>
<li><strong>Refresh database:</strong> Copies data from one environment to another (typically from production to a sandbox). <em>Remember, you can't copy production for reporting purposes.</em></li>
<li><strong>Export database:</strong> Saves a database from a sandbox to the Asset Library. The environment will be unavailable for servicing during this process.</li>
<li><strong>Import database:</strong> Loads a database from a developer environment or from a previously exported one.</li>
<li><strong>Point-in-time restore (PITR):</strong> Lets you revert to a previous state using automated backups (28 days for Production, 14 for Sandbox). This can also be done from production to a sandbox.</li>
</ul>
<p>The main thing to know here is that this is for Data Application Lifecycle Management (DataALM) and it provides a way to do refreshes for testing or other purposes. And remember that production data is not supported for reporting.</p>
<p>They also mention the possibility of moving databases between Tier-1 and Tier-2+ environments, which is for customer/partner managed environments, through a bacpac procedure (from SQL server to Azure SQL or vice versa) or by using SQL server backup/restore between one-box environments.</p>
<p>Next, we have the <strong>cross-company data sharing framework</strong>. This lets you share reference and group data between companies. It is important to note that this is not meant for transactional data. It's meant for things like payment terms and delivery terms that can be used across multiple legal entities. The key things to remember are:</p>
<ul>
<li>It is used to replicate (share) reference, parameter, and group data between companies within an environment.</li>
<li>It cannot be used for transactional data.</li>
<li>You have to ensure data integrity before you replicate.</li>
<li>The sharing policy should be enabled after importing data in the seeding company.</li>
<li>It's not meant for large, complex scenarios, such as franchising.</li>
<li>It should be tested in a sandbox before applying in production.</li>
<li>It cannot be used with Dual-write.</li>
</ul>
<p>They also mention a few other <strong>tools for data management</strong>: BYOD, Entity store refresh, Process data package, Data validation checklist workspace, and the Office add-in.</p>
<p>Then comes the next round of quiz questions:</p>
<ol>
<li><strong>Which one of the following data management concepts contains a data project manifest and data files?</strong> The answer here is <strong>Data package</strong>. Remember that this is a single compressed file that contains all elements of a data project.</li>
<li><strong>In which one of the following data management frameworks can you perform asynchronous integration by using data entities?</strong> This is <strong>Integration</strong>. Specifically, the asynchronous integration capabilities of the data management platform are used here.</li>
<li><strong>Which activity is performed using data entities?</strong> The correct choice is <strong>Pushing data from staging to target</strong>. Data entities are the mechanism to bring the staged data into the final target tables in Dynamics 365.</li>
<li><strong>For the import or export operation, users need to configure the entities that are being imported, define the default processing options, and define the mapping definition. Where are these operations performed?</strong> The answer is <strong>Data Projects</strong>. You configure the entities, default processing, and mapping inside the Data project.</li>
<li><strong>You need to allow access to your Customer records outside of finance and operations apps to be exposed in a Power App. Which of the following must be set up to make that work?</strong> The answer here is <strong>Data entity</strong>. You expose the Customer data via a data entity that Power Apps can then connect to.</li>
</ol>
<p>Finally, the last part of the module shifts to <strong>testing and user acceptance testing</strong>. Here, they introduce the importance of creating test cases, especially after customizing a solution or extending its functionality.</p>
<p>They talk about using the <strong>Task Recorder</strong> to record test cases. These can be organized in the <strong>Business Process Modeler (BPM)</strong>. They mention that you can also use BPM to distribute test libraries to customers through Lifecycle Services and across projects and teams. Also, BPM can synchronize with Azure DevOps, which can help in managing your testing processes.</p>
<p>They then describe the process of creating a <strong>BPM library</strong> by using either the BPM tile in Lifecycle Services or by importing using an Excel file. They also mention that BPM localization is not supported, so you need to synchronize with Visual Studio Team Server to see any changes made in US English.</p>
<p>And, finally, how to create and save new task recordings and save them to BPM or locally using AXTR files.</p>
<p>So, what did we learn? That data migration and management aren't just about moving data from A to B, but about understanding the entire ecosystem and the different tools available to you, and to test that everything works correctly. This whole module has covered data movement, transformation, and validation. It’s about having a holistic view of how data fits into Dynamics 365. And that is the most important part that the Microsoft exam is going to test you on. It's not just the how, but the why! Does this comprehensive overview help solidify your understanding?</p>
<p>Alright, let's bring this all home by covering the final pieces: recording guidelines, test synchronization, data task automation, and finally, go-live preparation.</p>
<p>First, the <strong>Edit Recording</strong> functionality is essential. It allows you to attach an existing task recording, save it directly to Lifecycle Services, or download it as an AXTR file and then upload it to BPM.</p>
<p>Then, they provide guidelines for recording test cases. These are crucial for writing test scripts, especially for automation:</p>
<ul>
<li><strong>Limited scope:</strong> Focus on small, specific business tasks, typically run by one person. Don't try to capture an entire end-to-end process in a single recording. Keep the tasks small. This makes it easier to maintain and run tests.</li>
<li><strong>Customized processes:</strong> Prioritize testing areas that have been customized.</li>
<li><strong>Validation:</strong> Include at least one validation step to check the impact of other fields.</li>
<li><strong>Screen selection:</strong> Avoid using "print" in test cases. Select reports on screen only.</li>
<li><strong>Transactions first:</strong> 80%+ of test cases should be focused on transactions or source documents, with master data being only around 20% of test cases.</li>
</ul>
<p>These are very important for exam prep, so make sure you remember them, along with the percentages.</p>
<p>They also mention the <strong>Regression Suite Automation Tool (RSAT)</strong>. It's mentioned here as another way to automate testing, but it does require Azure DevOps, although they mention you can also have a trial. They emphasize that RSAT significantly reduces the time and cost of testing and can be used to generate a full suite of test automation scripts, without coding.</p>
<p>Now, let's talk about <strong>synchronizing your test plans in Azure DevOps</strong>:</p>
<ul>
<li>You start with your BPM test library, which contains all your recorded test cases.</li>
<li>You then organize these cases into test plans and test suites in Azure DevOps.</li>
<li>You synchronize your BPM library with your Azure DevOps project (also referred to as VSTS). You can do that from the ellipsis button (...) on the Business process libraries page, and selecting the option "Azure DevOps sync (VSTS sync)".</li>
<li>After synchronization, select the ellipsis button and select "Sync Test Cases". Now, your recordings are test cases in Azure DevOps, and the XML file is attached to the test case.</li>
</ul>
<p>This whole process creates the base for your testing efforts in Azure DevOps.</p>
<p>Once you have completed synchronization, you can then create your test plan and test suites in Azure DevOps. They provide you the steps to create static test suites, add your test cases and create a test plan. After that, you can run the test cases manually, using the Test Runner tool, and mark them as passed or failed. And finally, they recommend using Azure DevOps, as it provides all features needed for testing, result management, and mitigation.</p>
<p>They also touch on <strong>automated test cases</strong>: Developers can write tests based on the recordings and use Azure DevOps for management. The Regression Suite Automation Tool (RSAT) is also mentioned again to simplify the creation of automated tests, mentioning that parameter files are in Excel, and that RSAT allows parallel execution of tests.</p>
<p>They also describe how you can investigate test runs from Azure DevOps.</p>
<p>Finally, they introduce <strong>data task automation</strong>. This is for repeating data tasks (such as creating data projects), configuration, activating import/export, and also for testing data entities. They recommend identifying potential tasks, defining those tasks in XML manifests, placing data packages in the Shared asset library, and then running and reviewing the outcomes. They also caution against running integration API-related tasks in production and recommend that this type of automation be done for testing purposes only.</p>
<p>They mention that the user running the data task automation must have access to Lifecycle Services and the Lifecycle Services project referenced in the manifest.</p>
<p>The lab exercise walks you through creating a BPM library, adding a new process and then sub-process, recording a test case using Task Recorder and saving it, either directly to Lifecycle Services or locally, for later upload.</p>
<p>Now for the last set of quiz questions:</p>
<ol>
<li><strong>Manage and distribute BPM libraries is part of which phase of the user acceptance tests processes in finance and operations apps?</strong> The answer is <strong>Create user acceptance test library</strong>. You create and manage BPM libraries as the first step in the testing process.</li>
<li><strong>Which one of the following considerations is part of the guidelines for recording test cases in finance and operations apps when you create a test case (recording)?</strong> The correct answer is <strong>A test case (recording) should cover one or two business tasks only and is typically performed by one person.</strong> This is about keeping recordings small and manageable.</li>
<li><strong>Which one of the following considerations is part of the guidelines when you create test cases and work with transactions and source documents?</strong> The correct answer is <strong>80+ percent of test cases should be from transactions or source documents. Master data should be limited to up to 20 percent of test cases.</strong></li>
<li><strong>When you run data task automation, which one of the following access controls rights?</strong> The answer is <strong>Access to Lifecycle Services and to the Lifecycle Services project that is referenced in the manifest for data packages</strong>. You need access to both Lifecycle Services and the specific project containing the data packages.</li>
</ol>
<p>So, we've covered the testing lifecycle, how to organize your tests, and the tools you can use.</p>
<p>And finally, the last bit of information in the module focuses on <strong>go-live preparation</strong>: They stress the importance of being organized and prepared for the go-live process.</p>
<p>They provide a sequence of steps that you should follow for go-live:</p>
<ul>
<li>Update the go-live date in Lifecycle Services. This should start 2-3 months prior to go-live.</li>
</ul>
<p>This module has been about preparing your data, testing your system, and preparing to launch. Remember the key concepts, the different tools, and most importantly, how they all fit together to ensure a successful Dynamics 365 implementation. Do you feel like you have a better grasp on everything now?</p>
<p>Okay, let's tackle this final section and bring this module to a close. This last part is all about the "go-live" process, which is the culmination of everything we've discussed so far. It's crucial to get this right!</p>
<p>The text starts with the <strong>go-live process</strong> itself. This is basically a checklist of actions:</p>
<ul>
<li><strong>Update the go-live date in Lifecycle Services:</strong> This is about keeping dates accurate and engaging with stakeholders. This should start 2-3 months prior to go-live.</li>
<li><strong>Complete and send the Pre go-live checklist:</strong> This is done after all UATs are signed off.</li>
<li><strong>Project assessment (FastTrack Essentials):</strong> A FastTrack architect reviews the project using your checklist.</li>
<li><strong>Project workshop (FastTrack):</strong> The FastTrack Architect coordinates assessment activities through a workshop.</li>
<li><strong>Release for production deployment:</strong> Done after a successful assessment.</li>
<li><strong>Request production deployment:</strong> This is done by the customer after the FastTrack assessment and is a self-service action.</li>
<li><strong>Submit deployable package installation request:</strong> You need to consider system downtime. 95% are usually done in an hour, but a 4-hour window is recommended.</li>
<li><strong>Submit request to copy database from sandbox (if applicable):</strong> You have to follow specific instructions. Also, note that a database copy has a lead time of 5 hours, and results in a 2-hour downtime.</li>
<li><strong>Production ready:</strong> You and the customer take control of the environment.</li>
<li><strong>Go-live:</strong> The customer goes live in production.</li>
</ul>
<p>So, these are sequential steps that build up to the actual go-live.</p>
<p>They also go into <strong>completing the Lifecycle Services methodology</strong>. This is a crucial step. Microsoft will only provision a production instance after the required activities in the methodology are completed. You need to have completed the Analysis, Design and develop, and Test phases.</p>
<p>The process of completing a phase is two-fold: perform the actual work, and select the corresponding step in Lifecycle Services as completed. It's recommended to complete the steps as you progress to avoid delays.</p>
<p>Next up is <strong>user acceptance testing (UAT)</strong>. Here are some key points:</p>
<ul>
<li>Test cases must cover the entire scope of requirements.</li>
<li>You should use migrated data (master data and opening balances).</li>
<li>Use the correct security roles.</li>
<li>Make sure the solution complies with regulations.</li>
<li>Document everything and get customer sign-off.</li>
</ul>
<p>It's also important to note that testing in Tier-1 environments is not enough. Testing must be done on a Tier-2 or higher sandbox. This is because Tier-1 topologies are different than that of production, they might lack critical aspects that are present in production. Also, system performance cannot be measured on a Tier-1 environment.</p>
<p>The text also touches on <strong>FastTrack go-live assessment</strong>. This is a mandatory review by Microsoft FastTrack before a production environment can be deployed. They provide implementation guidance, and access to Microsoft solution architects. They also mention FastTrack eligibility based on annual revenue, and how communications will happen (via solution architect if &gt; 300,000 USD, or via email otherwise).</p>
<p>You complete the go-live checklist (4-6 weeks before the go-live date), and then a Microsoft solution architect will review the project and provide an assessment, noting any risks. You cannot request the production environment until you have passed this assessment. If you request the production environment prematurely, it will remain in "Queued" status until the assessment is complete. You also learn that it is possible to cancel a deployment request by clearing the sign-off in the "Queued" status.</p>
<p>Finally, they explain how to <strong>request a production environment</strong>. They recommend using a service account as the admin user, and that the production environment should be deployed in the same datacenter as your sandbox environments. The Microsoft SLA for production environment deployments is 48 hours. The request goes into a "Queued" status, and after some time, changes to "Deploying". The Microsoft DSE team might add comments to the service request if they have questions that block the production deployment.</p>
<p>And now, for the final set of quiz questions:</p>
<ol>
<li>
<p><strong>Your customer is ready to go live and request to deploy the production environment. What must be done first?</strong> The correct answer is <strong>Complete a go-live review with the Microsoft FastTrack team.</strong> This is the required step <em>before</em> requesting the deployment of the production environment.</p>
</li>
<li>
<p><strong>Your customer is ready to go live and request to deploy the production environment by contacting the Microsoft FastTrack team. What happens next?</strong> The correct answer is <strong>The Microsoft FastTrack team reviews the project and provides an assessment.</strong> The FastTrack team does not deploy the environment.</p>
</li>
<li>
<p><strong>Your customer requests a copy database to be restored to a sandbox environment. Who is responsible for this process?</strong> The answer is <strong>Dynamics Service Engineering (DSE).</strong> DSE is responsible for the database movement operations.</p>
</li>
</ol>
<p>So, we've reached the end of the module! We've covered everything from planning data migration, choosing integration strategies, and setting up your testing processes, all the way to preparing for a successful go-live. This module really brings together all the components needed for a successful Dynamics 365 implementation. It also sets the stage for the exam by giving you insight into what the different processes and roles are for an implementation. I think we have covered every single detail required for the Microsoft exam in this module! Do you agree? Do you feel ready for the next step?</p>
<p>Okay, let's dive into this! It sounds like we're talking about how to get meaningful data out of Dynamics 365 Finance and Operations, which is super important because that's how businesses actually use the system. Forget about all the nitty-gritty details for a sec, let's focus on the big picture concepts so this all makes sense.</p>
<p>So, essentially, Dynamics 365 Finance and Operations isn't just a place to <em>store</em> data, it's a place to <em>use</em> data. This module is all about understanding the <em>different ways</em> we can pull that data out and turn it into something useful, like reports that help make informed decisions.</p>
<p>Think of it like this: you have a giant warehouse full of inventory (your data), but you need different ways to see what's in there depending on what you're trying to do. Sometimes you want to see the <em>big picture</em>, like overall revenue (that's the financial reporting piece), and sometimes you need the <em>details</em>, like an individual invoice (that’s where SSRS comes in).</p>
<p><strong>Key Idea 1: Variety of Reporting Tools</strong></p>
<p>The first thing to grasp is that Dynamics 365 isn't a "one-size-fits-all" for reporting. Instead, it gives you several specialized tools designed for different purposes. It's not just about picking <em>a</em> report; it's about choosing the <em>right</em> report for the job.</p>
<ul>
<li><strong>Embedded Analytics and Native Controls:</strong> This is basically the "quick glance" kind of reporting you see within the system. Think charts and grids built right into workspaces. It's great for getting a visual summary of what’s happening day to day.</li>
<li><strong>SQL Server Reporting Services (SSRS):</strong> This is your go-to for standard business documents like invoices, packing slips, and those things you need to print in high volumes. It's also useful for really detailed, customized reports you might want to schedule. SSRS lets you drill down to different areas of information and it's great for compliance documents for local regulations.</li>
<li><strong>Financial Report Designer:</strong> This is where you build those core financial statements like balance sheets, income statements, and cash flow statements. It’s very flexible and lets you customize how that financial data is presented.</li>
<li><strong>Microsoft Power Platform:</strong> This is the modern approach to reporting and analytics, which leverages Power BI dashboards and visualizations that come with Power Apps and Power Automate integrations. Think more interactive, visually appealing dashboards.</li>
<li><strong>Electronic Reporting:</strong> Forget developers – this tool is for the <em>business users</em>. It’s all about handling electronic documents with legal compliance in mind, making sure you're meeting regulations when sending out electronic information.</li>
<li><strong>Microsoft Office Integrations:</strong>  Super handy for taking data out of Dynamics 365 and putting it into Excel. You can export data to excel, edit and update the data, then publish those changes back into Dynamics 365.</li>
</ul>
<p><strong>Key Idea 2: Financial Reporting is Special</strong></p>
<p>The text really dives deep into the financial reporting piece. It's important to understand that this is specifically for creating and managing financial statements. It goes beyond the standard operational reports.</p>
<p>The key components are:</p>
<ul>
<li><strong>Report Designer:</strong> This lets you customize those financial reports, play with financial dimensions and organize information.</li>
<li><strong>Dimensions</strong>: Key to analyzing the information and understanding the context of the data (think cost centers, departments). Financial reporting lets you include these in different combinations to provide more insights.</li>
<li><strong>Row and Column Definitions</strong>: You're building blocks, determining <em>what</em> is on each row (like "Total Revenue" or "Salaries") and <em>what</em> is on each column (like "This Month", "Last Year").</li>
<li><strong>Reporting Tree Definition</strong>: Helps you represent the structure of the organization, see reports from the perspective of different departments or locations.</li>
<li><strong>Flexibility:</strong> This gives financial professionals the freedom to create reports that truly meet their needs by being flexible with how they use main accounts, financial dimensions, and combinations.</li>
</ul>
<p><strong>Key Idea 3: The Power of "Building Blocks"</strong></p>
<p>You'll see the term "building blocks" come up quite a bit. This is important because it emphasizes the modular nature of creating financial reports. You're not starting from scratch every time.</p>
<p>Row definitions, column definitions, reporting tree definitions, and report definitions can all be combined and reused across multiple reports which means you’re not reinventing the wheel every single time you need a different report.</p>
<p><strong>Key Idea 4: Inquiries and Reports are Everywhere</strong></p>
<p>Finally, it's important to remember that reports and inquiries are not just confined to the financial side. They're also built into other modules, such as Supply Chain Management and Human Resources. This means there are many out-of-the-box reports you can use without having to build them yourself.</p>
<p><strong>So, how does this relate to the certification?</strong></p>
<p>For the exam, you don’t need to memorize every single detail about how to create a report. What you <strong>do</strong> need to understand is the <em>purpose</em> of each type of reporting tool. Be familiar with the concept of building blocks, especially related to Financial Reporting, and understand the difference between a basic operational report versus a standard financial statement.</p>
<p>Also, be sure to know what types of questions each tool is best suited for, and be ready to think critically about what kind of reporting solution you'd choose for different business needs.</p>
<p>Does that all make sense? Let me know if you have any other questions or if there is anything else you’d like to explore.</p>
<p>Alright, let's break down this next chunk of info! It's building on what we've already discussed, moving from basic reporting to more advanced capabilities. Again, we are not sweating the small stuff, just focusing on the key concepts.</p>
<p><strong>Key Idea 1: Document Preview and Printing Enhancements</strong></p>
<p>The first thing to note is how Dynamics 365 is handling document previews and printing. Think of it as making the whole process a lot more user-friendly.</p>
<ul>
<li><strong>Preview Before Export:</strong>  Most users want to see how a document looks before they export or print it. The system now has consistent toolbar options for previewing documents on various devices.</li>
<li><strong>Consistent Export Formats:</strong> The new toolbar actions convert documents into a format that's more consistent when downloaded compared to the old HTML-based Report Viewer.</li>
<li><strong>Network Printer Support:</strong> It's easier to send documents directly to network printers supported by Dynamics 365, which means users can print remotely from their mobile devices or computers.</li>
</ul>
<p>Basically, it's all about making it simpler and more reliable to get reports and documents out of the system.</p>
<p><strong>Key Idea 2: Power Platform Integration</strong></p>
<p>Now, let’s move to the really exciting stuff: the integration with the Microsoft Power Platform. This is where the reporting capabilities of Dynamics 365 really expand!</p>
<ul>
<li><strong>Workspaces with Interactive Reports:</strong> Think of workspaces as dashboards that use visuals from Power BI. This is more interactive and visually appealing than traditional reporting.</li>
<li><strong>Power Apps:</strong> You can embed Power Apps into Dynamics 365 to create custom applications, which is a way to tailor the system to the specific needs of your business. The key here is that you are using the same credentials across both platforms. Power Apps is great for building apps quickly with a visual approach.</li>
<li><strong>Power Automate:</strong> This is for automation and workflow. You can create workflows that trigger when events happen in Dynamics 365. This can do things like send notifications, sync files, or collect data. It's all about automating business processes.</li>
<li><strong>Power BI:</strong> As we've discussed, this is for interactive dashboards and reports. Dynamics 365 has pre-built Power BI content packs that you can use as templates for your own Power BI dashboards.</li>
</ul>
<p><strong>Key Idea 3: Pre-Built Power BI Content Packs</strong></p>
<p>This is a pretty big deal. The text lists out <em>tons</em> of pre-built Power BI content packs for different areas of the business. These are essentially templates you can use to get started with Power BI in Dynamics 365. The key thing to understand is how these solutions provide insights for various business functions:</p>
<ul>
<li><strong>Financial:</strong> Actual vs. budget, cash overview, financial analysis</li>
<li><strong>HR:</strong> Compensation and benefits, workforce metrics, employee competencies</li>
<li><strong>Operations:</strong> Cost accounting analysis, fixed asset management, practice manager, cost management, production performance, purchase spend analysis</li>
<li><strong>Sales and Marketing:</strong> Sales and profitability performance, credit and collections management, vendor payments, warehouse performance</li>
<li><strong>Recruiting</strong>: Recruiting</li>
</ul>
<p>These content packs are all about using existing data in Dynamics 365 to get a holistic view of different areas of the business. They also emphasize the need for setup to be correct such as for liabilities and revenue amounts to show as positive, the account type must be set to "Liability" or "Revenue".</p>
<p><strong>Key Idea 4: The Lab Exercise</strong></p>
<p>The lab exercise is a hands-on demonstration of how to generate and explore a financial report in Dynamics 365, changing the dimension order, drilling into account details, applying dimension filters, and switching the currency. It reinforces all the concepts we’ve already covered.</p>
<p><strong>Key Idea 5: Electronic Reporting (ER) Introduction</strong></p>
<p>Finally, the introduction to Electronic Reporting (ER) is given. It emphasizes this functionality as the primary standard tool to support localization. The module highlights that ER allows business users to maintain electronic document formats through a configuration process.</p>
<p><strong>So, what's the big takeaway for the exam?</strong></p>
<ul>
<li><strong>Power Platform Integration:</strong> Understand how Power BI, Power Apps, and Power Automate are integrated with Dynamics 365 and how they can extend its reporting capabilities. Know the different ways they can be used.</li>
<li><strong>Power BI Content Packs:</strong>  Be aware of the types of content packs that are available and what areas of the business they cover. You don't need to memorize all of them, but be familiar with the concept.</li>
<li><strong>Practical Application:</strong> Understand the purpose of the lab and how it relates to the key concepts that you've been learning. Be able to apply this logic to different scenarios you might encounter in the exam.</li>
<li><strong>Electronic Reporting:</strong> Understand ER's purpose and that it is meant to be used by the business user and is a more maintainable solution as opposed to having to code the solution.</li>
</ul>
<p>The exam is not going to be asking you to perform the steps of the lab, but it will ask about these concepts. Think conceptually!</p>
<p>Does this explanation make sense? Are you ready to tackle more material?</p>
<p>Okay, let's tackle this final section. It's all about Electronic Reporting (ER) and its practical application with a lab exercise.</p>
<p><strong>Key Idea 1: ER Setup</strong></p>
<p>The beginning focuses on setting up ER, and again, the focus is on <em>understanding what each part does</em>, not necessarily memorizing each step. Here’s the gist:</p>
<ul>
<li><strong>Document Types in Document Management:</strong> Before you can use ER, you need to set up the document types that will be used for various purposes, including templates, output storage, baselines, etc.</li>
<li><strong>ER Parameters:</strong> In the ER workspace, you will find the settings to configure the design mode, attachments, and lifecycle services. These are configurations that are set up in the general, attachments, and lifecycle services tabs.</li>
<li><strong>Configuration Providers:</strong> ER configurations come from a "provider." It's like a source or a vendor that provides the format configurations. You need to set one as active, which basically tells the system, "This is where we get our ER configurations from." You can create your own, if needed and they are automatically shared across all legal entities.</li>
</ul>
<p><strong>Key Idea 2: Legal Entity-Specific Parameters</strong></p>
<p>This section emphasizes ER's flexibility when it comes to company-specific needs.</p>
<ul>
<li><strong>Corporate Rules:</strong> You can define corporate rules that are translated into different languages, allowing business users to specify terms and conditions in their language of choice.</li>
<li><strong>Legal Entity-Specific Data:</strong> If you have a company-specific rule configured, a field for the data source returns values describing the configured rule, and the system will allow navigation to it. This helps business users specify what parts of the search data will be provided to them.</li>
<li><strong>Legal Entity-Independent Rules:</strong> This allows you to define rules that return enumeration data types for each condition or a text value.</li>
</ul>
<p><strong>Key Idea 3: Barcodes and Check Number Validation</strong></p>
<p>This section touches on some of the practical uses of ER.</p>
<ul>
<li><strong>Barcode Data Sources:</strong>  ER lets you create barcode images from data sources. This means you can add barcodes to all sorts of documents: invoices, packing slips, and even labels. ER supports both one-dimensional and two-dimensional barcodes.</li>
<li><strong>Check Number Validation:</strong>  You can validate check numbers, reducing the risk of errors. The system uses either interval validation which checks if the check number is within a range, or a character validation that triggers when a specific character appears in the check number.</li>
</ul>
<p><strong>Key Idea 4: The ER Lab Exercise</strong></p>
<p>The lab is really practical:</p>
<ul>
<li><strong>Scenario:</strong> You're playing the role of a finance manager setting up ER to meet a local requirement.</li>
<li><strong>Steps:</strong> You test the format by generating a file to make sure that it meets the requirements. This means you have to verify the file format, then generate the payment and see that the format is what you want. This can be checked when you have opened the file in Notepad.</li>
</ul>
<p>This lab is not only to make sure that the data is being generated as it should but also that it's an efficient solution. It's all about validating that the configurations are working as expected.</p>
<p><strong>Key Idea 5: ER Capabilities Summary</strong></p>
<p>The module summarizes the capabilities of Electronic Reporting. ER:</p>
<ul>
<li>Supports different file formats like TEXT, XML, PDF, and OPENXML spreadsheets.</li>
<li>Is designed for business users with Excel-based formulas.</li>
<li>Easily adheres to changing regulatory requirements.</li>
<li>Has versioning to manage the lifecycle.</li>
<li>Is used for electronic invoicing.</li>
</ul>
<p><strong>What's important for the exam?</strong></p>
<ul>
<li><strong>ER Setup:</strong> Understand the key settings in the Electronic reporting workspace, how to set up configuration providers, and where the document management setup comes into play.</li>
<li><strong>ER Purpose and Benefits:</strong> Know what ER is designed for (managing electronic documents), who is supposed to use it (business users), and the benefits it provides (flexibility, adherence to regulatory changes, versioning, and more).</li>
<li><strong>Practical Applications:</strong> Recognize that ER isn't just about creating files, but that it's used for things like barcodes and check number validation.</li>
<li><strong>Lab Scenario:</strong> Understand the scenario in the lab and how the practical steps link back to the concepts you've been learning.</li>
</ul>
<p>The key here is to not get lost in the weeds of setting up <em>every</em> little thing. Focus on the <em>why</em> and the <em>what</em>. Why is ER important? What are its key capabilities? Who is supposed to use it?</p>
<p>And with that, we've covered the whole module! How are you feeling about all of this? Ready to move on or do you have any final questions before we wrap things up?</p>
    </body>
</html>